{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dbb3508",
   "metadata": {},
   "source": [
    "# TIL: Left vs Right Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419e039a",
   "metadata": {},
   "source": [
    "> TL;DR You should use **left padding** for autoregressive LLMs (GPT2), but **right padding** for encoder LLMs (BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43a7cac",
   "metadata": {},
   "source": [
    "## Why do we need padding?\n",
    "\n",
    "Language models expect a tensor of shape `(batch_size, sequence_length)`, therefore each sequence in the batch must be of the same length.\n",
    "\n",
    "However not all input texts are of the same size (obviously :))\n",
    "\n",
    "Let us assume that each word in a sequence is it's own token, and `[CLS]` represents the start of text token.\n",
    "\n",
    "We could not pass both these texts at the same time:\n",
    "\n",
    "```\n",
    "[CLS]  hello  world                               (length 3)\n",
    "[CLS]  the     cat     sat    on     the    mat   (length 7)\n",
    "```\n",
    "\n",
    "Therefore we must _pad_ the smaller sequences up to the same size as the longest sequence, which brings us to -- do we pad the _left_ side of the sequence or the _right_?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "538338ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", add_bos_token=True)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]', 'bos_token': '[CLS]'});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0baf00f",
   "metadata": {},
   "source": [
    "## Right Padding\n",
    "\n",
    "This seems the most intuitive, we pad the right side of the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3da3450d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]  | hello  |  world | [PAD]  | [PAD]  | [PAD]  | [PAD] \n",
      "[CLS]  | the    |  cat   |  sat   |  on    |  the   |  mat  \n"
     ]
    }
   ],
   "source": [
    "prompts = [\"hello world\", \"the cat sat on the mat\"]\n",
    "\n",
    "tokenizer.padding_side = \"right\"\n",
    "batch_tokens = tokenizer(prompts, padding=\"longest\").input_ids\n",
    "for tokens in batch_tokens:\n",
    "    print(\" | \".join(f\"{tokenizer.decode([token]):6}\" for token in tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d397d0e7",
   "metadata": {},
   "source": [
    "## Left Padding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4668ffae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD]  | [PAD]  | [PAD]  | [PAD]  | [CLS]  | hello  |  world\n",
      "[CLS]  | the    |  cat   |  sat   |  on    |  the   |  mat  \n"
     ]
    }
   ],
   "source": [
    "tokenizer.padding_side = \"left\"\n",
    "batch_tokens = tokenizer(prompts, padding=\"longest\").input_ids\n",
    "for tokens in batch_tokens:\n",
    "    print(\" | \".join(f\"{tokenizer.decode([token]):6}\" for token in tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaab1a6",
   "metadata": {},
   "source": [
    "## Encoder vs Decoder Models\n",
    "\n",
    "### Encoder\n",
    "\n",
    "For _encoder_ style models (BERT) we extract the final latent representation of the first `[CLS]` token at the start of the sequence. Due to full attention, this includes all the information from the entire sequence.\n",
    "\n",
    "For encoder models, right padding makes sense, the `[CLS]` token is always in the first position and makes it easy to extract.\n",
    "\n",
    "```python\n",
    "residual_output: Float[Tensor, \"batch seq d_model\"]\n",
    "cls_resid = residual[:, 0, :] # extract [CLS] residual (index 0)\n",
    "```\n",
    "\n",
    "```\n",
    "extract\n",
    "  |\n",
    "  v\n",
    "[CLS]  hello  world  [PAD]  [PAD]\n",
    "```\n",
    "\n",
    "### Decoder\n",
    "\n",
    "\n",
    "For _decoder_ style models (GPT2) we want to autoregressively generate the next token based on the final token in the sequence. \n",
    "\n",
    "```python\n",
    "residual_output: Float[Tensor, \"batch seq d_model\"]\n",
    "final_tok_resid = residual[:, -1, :] # extract residual of final token (to predict the next one)\n",
    "logits = W_U @ final_tok_resid\n",
    "```\n",
    "\n",
    "Padding before the bos token (`[CLS]`) allows our model to ignore the variable length padding when autoregressively generating an output.\n",
    "\n",
    "```\n",
    "[PAD]  [PAD]  [CLS]  hello  world  ->  (next token prediction)\n",
    "```\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
