{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fec8c7c-5c5b-4582-be0d-2aac483d46f1",
   "metadata": {},
   "source": [
    "# Sampling From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "3f7f433c-c671-4122-a066-765af2bb0490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, tensor\n",
    "from jaxtyping import Float, Int\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "torch.set_grad_enabled(False); # disable backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a212d0-a163-4b64-b42d-1a4698064fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, local_files_only=True).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, local_files_only=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9e6633-b55a-4a90-abd9-3050bebf56e9",
   "metadata": {},
   "source": [
    "LLMs do not operate on words -- each word is converted into a high dimensional vector that contains information that gets passed through the model. At each layer, the model reads the vector, performs some computation (attention or MLP) and writes it back to the vector.\n",
    "\n",
    "We call this vector the **residual stream**. To initially create these vectors from a sentence, we have a large lookup table of each \"word\" (or sub-word, see [here](TODO) for more info) to a this high dimensional vector.\n",
    "\n",
    "> We call each \"word\" a **token**.  \n",
    "> You can imagine `token ~= word`\n",
    "\n",
    "This is 768 dimensions on GPT2, and can also be thought of the _width_ of the model\n",
    "\n",
    "_depth_ being the number of layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafde6a7-d46b-4cce-8da9-8889062e2fac",
   "metadata": {},
   "source": [
    "We look up each word in an _embedding_ table. This is a map of 50,000 words to a high dimensional embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a6cd4bee-7430-4552-8245-84c825968f07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50257, 768)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.wte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6b5b79-0710-4d7c-b1cb-8fc5d0144cea",
   "metadata": {},
   "source": [
    "Let's see the first 10 dimensions of the token (word) 9246"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "a2a10d26-abc6-4dfc-b158-647aa703e9d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0164, -0.0934,  0.2425,  0.1398,  0.0388, -0.2592, -0.2724, -0.1625,\n",
       "         0.1683,  0.0829], device='mps:0', requires_grad=True)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = 9246\n",
    "first_n_dimensions = 10\n",
    "model.transformer.wte.weight[token, :first_n_dimensions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6724c01-a02a-4b63-923e-fb56f0ca1b00",
   "metadata": {},
   "source": [
    "And to find the corresponding string word associated with token 9246:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "aa29f7a6-3f24-40be-9146-1baf44484c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoded token: 'cat'\n"
     ]
    }
   ],
   "source": [
    "def decode(tokens) -> str:\n",
    "    return tokenizer.decode(tokens)\n",
    "\n",
    "print(f\"decoded token: {repr(decode(token))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e2a6f3-78dc-4ec1-a59a-0009c3cf7388",
   "metadata": {},
   "source": [
    "Using the `tokenize` and `decode` functions, we can convert back and forth between a string and the initial model vectors (\"embeddings\").\n",
    "\n",
    "Notably the model adds a \"batch\" dimension to the input, which allows us to process multiple inputs at the same time, imagine this allows us to run \"the cat sat on the mat\" and \"I took my dog for a walk\" at the _same time_.\n",
    "\n",
    "Input to a LLM is a list of tokens, which we call length sequence length (or `seq` / `T` (for time dimesion) for short.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "a70346c1-7659-409f-b128-c3b24148e267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# prompt\n",
      "the cat sat on a mat\n",
      "\n",
      "# tokens shape: (1, 6)\n",
      "[[1169, 3797, 3332, 319, 257, 2603]]\n",
      "\n",
      "# decoded\n",
      "the cat sat on a mat\n",
      "\n",
      "# embeddings shape: (1, 6, 768)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize(input) -> Int[Tensor, \"bs seq\"]:\n",
    "    return tokenizer(\n",
    "        input,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=model.config.n_ctx,\n",
    "    )[\"input_ids\"].to(device)\n",
    "\n",
    "prompt = 'the cat sat on a mat'\n",
    "tokens = tokenize(prompt)\n",
    "embeddings = model.transformer.wte.weight[tokens]\n",
    "\n",
    "decoded = decode(tokens[0])\n",
    "\n",
    "print(f\"\"\"\\\n",
    "# prompt\n",
    "{prompt}\n",
    "\n",
    "# tokens shape: {tuple(tokens.shape)}\n",
    "{tokens.tolist()}\n",
    "\n",
    "# decoded\n",
    "{decoded}\n",
    "\n",
    "# embeddings shape: {tuple(embeddings.shape)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74705ade-01f9-41e6-8b3d-efe6e39acd8e",
   "metadata": {},
   "source": [
    "## Output\n",
    "\n",
    "Now given the prompt input, lets run the tokens through the model and look at the output. These are called **logits**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "86f0611e-c430-4281-9ed4-05896ee77f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tokens ((1, 6))\n",
      "\n",
      "# Logit Output ((1, 6, 50257))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logits = model(tokens).logits\n",
    "\n",
    "print(f\"\"\"\\\n",
    "# Tokens ({tuple(tokens.shape)})\n",
    "\n",
    "# Logit Output ({tuple(logits.shape)})\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99055bb-2211-4ddd-8ba8-884970cb6a28",
   "metadata": {},
   "source": [
    "The input has shape, `(batch size, sequence length)`, with output `(batch size, sequence length, logits)`\n",
    "\n",
    "For each token in the sequence, the model outputs a score for _every next token_ (50K) representing how likely that token is to come next.\n",
    "\n",
    "For each token, we can see which token the model predicted as _most likely_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "83193811-58a3-4570-a4b8-82988e633b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'the' => ','\n",
      "'the cat' => ','\n",
      "'the cat sat' => ' on'\n",
      "'the cat sat on' => ' the'\n",
      "'the cat sat on a' => ' bench'\n",
      "'the cat sat on a mat' => ','\n"
     ]
    }
   ],
   "source": [
    "for i in range(tokens.shape[1]):\n",
    "    inp = decode(tokens[0, :i+1])\n",
    "    pred = decode(logits[0, i].argmax())\n",
    "    print(f\"{repr(decode(tokens[0, :i+1]))} => {repr(pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4c9d83-388a-4197-b755-5b61904f900d",
   "metadata": {},
   "source": [
    "So to continue generating tokens, we need to run an **auto regressive** function, that selects a token from the _last_ word in the sequence, and append it to the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "84bdb47a-4d6e-48c0-a9c3-4bd0d3ea8647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the cat sat on a mat,\n",
      "the cat sat on a mat, and\n",
      "the cat sat on a mat, and the\n",
      "the cat sat on a mat, and the cat\n",
      "the cat sat on a mat, and the cat sat\n",
      "the cat sat on a mat, and the cat sat on\n",
      "the cat sat on a mat, and the cat sat on a\n",
      "the cat sat on a mat, and the cat sat on a mat\n",
      "the cat sat on a mat, and the cat sat on a mat,\n",
      "the cat sat on a mat, and the cat sat on a mat, and\n",
      "the cat sat on a mat, and the cat sat on a mat, and the\n",
      "the cat sat on a mat, and the cat sat on a mat, and the cat\n",
      "the cat sat on a mat, and the cat sat on a mat, and the cat sat\n",
      "the cat sat on a mat, and the cat sat on a mat, and the cat sat on\n",
      "the cat sat on a mat, and the cat sat on a mat, and the cat sat on a\n",
      "the cat sat on a mat, and the cat sat on a mat, and the cat sat on a mat\n",
      "the cat sat on a mat, and the cat sat on a mat, and the cat sat on a mat,\n",
      "the cat sat on a mat, and the cat sat on a mat, and the cat sat on a mat, and\n",
      "the cat sat on a mat, and the cat sat on a mat, and the cat sat on a mat, and the\n",
      "the cat sat on a mat, and the cat sat on a mat, and the cat sat on a mat, and the cat\n"
     ]
    }
   ],
   "source": [
    "def generate(prompt, num_tokens, verbose=False):\n",
    "    tokens = tokenize(prompt)\n",
    "    for i in range(num_tokens):\n",
    "        logits = model(tokens).logits[0, -1] # get the scores of the final token [shape: (n_vocab)]\n",
    "        next_token = logits.argmax(keepdim=True) # pick the largest one\n",
    "        tokens = torch.cat([ tokens, next_token[None] ], dim=1) # concatenate to the current text\n",
    "        if verbose:\n",
    "            print(decode(tokens[0]))\n",
    "    return decode(tokens[0])\n",
    "\n",
    "generate(prompt, num_tokens=20, verbose=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3c58eb-5bce-4710-9081-76ea79bacf6a",
   "metadata": {},
   "source": [
    "## Sampling Probability Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660d4005-6523-42d5-8119-a54f515be62f",
   "metadata": {},
   "source": [
    "But just picking the most likely can give quite bland output\n",
    "\n",
    "This takes the model output (which can be any number) and create a _probability distribution_ such that all the scores add up to 1.\n",
    "\n",
    "To do this we use the **softmax** function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "a23a75b2-0f0d-45d9-b51a-b9bd78fb37c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the cat sat on a mat and\n",
      "the cat sat on a mat and did\n",
      "the cat sat on a mat and did something\n",
      "the cat sat on a mat and did something which\n",
      "the cat sat on a mat and did something which,\n",
      "the cat sat on a mat and did something which, oddly\n",
      "the cat sat on a mat and did something which, oddly enough\n",
      "the cat sat on a mat and did something which, oddly enough,\n",
      "the cat sat on a mat and did something which, oddly enough, most\n",
      "the cat sat on a mat and did something which, oddly enough, most ordinary\n",
      "the cat sat on a mat and did something which, oddly enough, most ordinary folk\n",
      "the cat sat on a mat and did something which, oddly enough, most ordinary folk never\n",
      "the cat sat on a mat and did something which, oddly enough, most ordinary folk never do\n",
      "the cat sat on a mat and did something which, oddly enough, most ordinary folk never do!\n",
      "the cat sat on a mat and did something which, oddly enough, most ordinary folk never do! )\n",
      "the cat sat on a mat and did something which, oddly enough, most ordinary folk never do! ) It\n",
      "the cat sat on a mat and did something which, oddly enough, most ordinary folk never do! ) It is\n",
      "the cat sat on a mat and did something which, oddly enough, most ordinary folk never do! ) It is 137\n",
      "the cat sat on a mat and did something which, oddly enough, most ordinary folk never do! ) It is 137E\n",
      "the cat sat on a mat and did something which, oddly enough, most ordinary folk never do! ) It is 137EUNE\n"
     ]
    }
   ],
   "source": [
    "def generate(prompt, num_tokens, verbose=False, seed=42): # add a seed to keep the output deterministic. Try other seeds!\n",
    "    torch.manual_seed(seed)\n",
    "    tokens = tokenize(prompt)\n",
    "    for i in range(num_tokens):\n",
    "        logits = model(tokens).logits[0, -1]\n",
    "        ### New lines\n",
    "        probs = F.softmax(logits, dim=-1) # create probability distribution of scores\n",
    "        next_token = torch.multinomial(probs, 1) # pick a single token from distribution\n",
    "        ###\n",
    "        tokens = torch.cat([ tokens, next_token[None] ], dim=1)\n",
    "        if verbose:\n",
    "            print(decode(tokens[0]))\n",
    "    return decode(tokens[0])\n",
    "\n",
    "generate(prompt, num_tokens=20, verbose=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a881d6-5278-4219-8f39-bc6c5d8c3e81",
   "metadata": {},
   "source": [
    "This already gives a much more interesting output! But perhaps we want to control \n",
    "\n",
    "Now how can we _control_ how much of the distribution we sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946758b1-e393-4a43-b3e9-2876612250ae",
   "metadata": {},
   "source": [
    "## Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04e1a8f-6752-4383-9269-b945712c3983",
   "metadata": {},
   "source": [
    "**Temperature** controls how the distribution is sampled. It's best shown in the context of the examples above\n",
    "\n",
    "- Temperature 0: Completely flattens the distrubution, all probability is given to the token with the largest score\n",
    "- Temperature 1: Standard softmax distrubution, same as sampling above\n",
    "\n",
    "By increasing the temperature, we increase the chance of a token with a lower probability getting picked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "50e439c6-8004-421e-8f5a-c886e8a167ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### 0.0 ###\n",
      "the cat sat on a mat, and the cat sat on a mat, and the cat sat on a mat, and the cat\n",
      "\n",
      "### 0.2 ###\n",
      "the cat sat on a mat and she was eating a bowl of rice. The cat was so hungry that she had to be fed\n",
      "\n",
      "### 0.4 ###\n",
      "the cat sat on a mat and she was screaming.\n",
      "\n",
      "\"I don't know what to do,\" she said.\n",
      "\n",
      "\n",
      "### 0.6 ###\n",
      "the cat sat on a mat and she looked at me with a smile and said, \"by the way, I'm going home\n",
      "\n",
      "### 0.8 ###\n",
      "the cat sat on a mat and did something which was oddly fitting. It did not sit well!\n",
      "\n",
      "I think I tried\n",
      "\n",
      "### 1.0 ###\n",
      "the cat sat on a mat and did something which, oddly enough, most ordinary folk never do! ) It is 137EUNE\n",
      "\n",
      "### 1.2 ###\n",
      "the cat sat on a mat and did something which none of us appreciated working for me -by Roy Cairity 13717 walks\n",
      "\n",
      "### 1.4 ###\n",
      "the cat sat on a mat and did something predictably right Floracles understandably squirmed excited -by Roy Collins 403 paths 137 moves 15\n",
      "\n",
      "### 1.6 ###\n",
      "the cat sat on a mat and did Sunshade Floracles instead game day morning -by Roy Collins 403 paths 137 moves dealt\n",
      "\n",
      "### 1.8 ###\n",
      "the cat sat on a mat and did Sunshade Floracles alternating physics workwitch syllby Roy Collins 403 paths 137 moves dealt\n",
      "\n",
      "### 2.0 ###\n",
      "the cat sat on a mat token (~nil predictably right Poké GraphPlex physics proofwitch botby lived broadcast 403 paths 137 moves dealt\n"
     ]
    }
   ],
   "source": [
    "def generate(prompt, num_tokens, temperature=1.0, seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    tokens = tokenize(prompt)\n",
    "    temperature = max(temperature, 1e-8) # temperature 0 => divide by _very small_ constant\n",
    "    for i in range(num_tokens):\n",
    "        logits = model(tokens).logits[0, -1]\n",
    "        probs = F.softmax(logits / temperature, dim=-1) # divide scores, flattening distribution\n",
    "        next_token = torch.multinomial(probs, 1)\n",
    "        tokens = torch.cat([ tokens, next_token[None] ], dim=1)\n",
    "    return decode(tokens[0])\n",
    "\n",
    "for temp in torch.arange(0, 2.2, 0.2):\n",
    "    print(f\"\\n### {temp.item():.1f} ###\")\n",
    "    print(generate(prompt, num_tokens=20, temperature=temp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919ca839-00ad-4023-b4bb-232b2159475f",
   "metadata": {},
   "source": [
    "As the temperature increases, less likely tokens are predicted, which can lead to more interesting output. Setting the temperature hyperparameter correctly can be key to model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0d95f0-0283-409f-b2b4-0d3d1ab0a324",
   "metadata": {},
   "source": [
    "## Top K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4160cd2-839e-4d31-8dee-0be50b96e438",
   "metadata": {},
   "source": [
    "Another parameter used in sampling is `top_k`. This essentially limits the model predicting too \"wild\" predictions by limiting the probability distribution to the top k results.\n",
    "\n",
    "A.k.a currently we are sampling from the entire distribution of 50,000 tokens. But it makes sense that only the top 50 tokens are reasonable continuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "2071da2f-44c0-42aa-a83a-f642042ab0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Temperature 0.0 ###\n",
      "the cat sat on a mat, and the cat sat on a mat, and the cat sat on a mat, and the cat\n",
      "\n",
      "### Temperature 0.2 ###\n",
      "the cat sat on a mat, and the cat was sitting on the mat.\n",
      "\n",
      "\"I thought, 'Oh, I\n",
      "\n",
      "### Temperature 0.4 ###\n",
      "the cat sat on a mat.)\n",
      "\n",
      "\"This is what you have to do,\" said the boy. \"You have to\n",
      "\n",
      "### Temperature 0.6 ###\n",
      "the cat sat on a mat.)\n",
      "\n",
      "\"They're just trying to get him to stop,\" says one visitor. \"I\n",
      "\n",
      "### Temperature 0.8 ###\n",
      "the cat sat on a mat.)\n",
      "\n",
      "\"Yeah, you know, I thought it was a nice day last night (laughs\n",
      "\n",
      "### Temperature 1.0 ###\n",
      "the cat sat on a mat.)\n",
      "\n",
      "If that's all it takes, well, that's how I love it that you\n",
      "\n",
      "### Temperature 1.2 ###\n",
      "the cat sat on a mat.)\n",
      "\n",
      "If that's all it takes, well, that's how I love him the way\n",
      "\n",
      "### Temperature 1.4 ###\n",
      "the cat sat on a mat.)\n",
      "\n",
      "If such a man had had one word to say to me about love of Christ:\n",
      "\n",
      "### Temperature 1.6 ###\n",
      "the cat sat on a mat.)\n",
      "\n",
      "If such a man had had one word to say to me about love of Christ:\n",
      "\n",
      "### Temperature 1.8 ###\n",
      "the cat sat on a mat.)\n",
      "\n",
      "If such a man had had one word to give to this matter his friend must be\n",
      "\n",
      "### Temperature 2.0 ###\n",
      "the cat sat on a mat.)\n",
      "\n",
      "If such a man had called us one day while we was still being called names in\n"
     ]
    }
   ],
   "source": [
    "def generate(prompt, num_tokens, temperature=1.0, top_k=50, seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    tokens = tokenize(prompt)\n",
    "    temperature = max(temperature, 1e-8)\n",
    "    for i in range(num_tokens):\n",
    "        logits = model(tokens).logits[0, -1]\n",
    "        if top_k:\n",
    "            logits, idxs = logits.topk(top_k) # Sample only topk tokens\n",
    "        else:\n",
    "            idxs = torch.arange(len(logits), device=device) # All idxs\n",
    "    \n",
    "        probs = F.softmax(logits / temperature, dim=-1)\n",
    "        next_token = idxs[torch.multinomial(probs, 1)] # we use the idxs of topk only\n",
    "        tokens = torch.cat([ tokens, next_token[None] ], dim=1)\n",
    "\n",
    "        tokens = torch.cat([ tokens, next_token[None] ], dim=1)\n",
    "    return decode(tokens[0])\n",
    "\n",
    "for temp in torch.arange(0, 2.2, 0.2):\n",
    "    print(f\"\\n### Temperature {temp.item():.1f} ###\")\n",
    "    print(generate(prompt, num_tokens=20, temperature=temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16345a67-46c3-40b5-8ad8-9d0d8f3d5fdb",
   "metadata": {},
   "source": [
    "You can see at even very high temperatures, the output does not devolve into gibberish."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e4abfc-5075-4d8b-8c93-2a2828fae8c8",
   "metadata": {},
   "source": [
    "## Min P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e56a78-81b3-4150-993a-12915c849f4b",
   "metadata": {},
   "source": [
    "Top K can often be a to naive heuristic for sampling. A more common technique nowdays is to instead dispose of tokens that have too low probability.\n",
    "\n",
    "We do this by computing the fraction of the of the probability of a token compared to the most probable token.\n",
    "\n",
    "A.k.a If the most probable token has 60% proability and we have `min_p = 0.1`, we dispose of all tokens with a probability less than 6%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "85109728-88fc-4cba-b3d9-5cadb8faad77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Min P: 0.50 ###\n",
      "the cat sat on a mat and the cat sat on a bench. The cat was sitting on a bench.\n",
      "\n",
      "\"You\n",
      "\n",
      "### Min P: 0.32 ###\n",
      "the cat sat on a mat with a large bag of water in it, and she had been in the water for a long time\n",
      "\n",
      "### Min P: 0.21 ###\n",
      "the cat sat on a mat with a small tray on the top, while the dog stood up, looking up at the cat and\n",
      "\n",
      "### Min P: 0.14 ###\n",
      "the cat sat on a mat that had been planted over a tall wall and its claws were stuck to the floor. She tried to\n",
      "\n",
      "### Min P: 0.09 ###\n",
      "the cat sat on a mat and told him to hold it down. But he shook his head, \"Don't get involved,\n",
      "\n",
      "### Min P: 0.06 ###\n",
      "the cat sat on a mat and my cock was about a foot up from her anus. She walked back to my room and let\n",
      "\n",
      "### Min P: 0.04 ###\n",
      "the cat sat on a mat in a hospital bed beside a bed-covered seat.\"\n",
      "\n",
      "\"It's such a strong feeling\n",
      "\n",
      "### Min P: 0.02 ###\n",
      "the cat sat on a mat\n",
      "\n",
      "Until God said, \"Maybe it's best you don't be dead before you let the\n",
      "\n",
      "### Min P: 0.02 ###\n",
      "the cat sat on a mat: more reason to dig!\n",
      "\n",
      "Joaquim Jimenez joined the rest of us over a\n",
      "\n",
      "### Min P: 0.01 ###\n",
      "the cat sat on a mat: I trust our ability to keep an eye on my kid and claim her your autograph this late\n"
     ]
    }
   ],
   "source": [
    "def generate(\n",
    "    prompt,\n",
    "    num_tokens,\n",
    "    temperature=1.0,\n",
    "    top_k=None,\n",
    "    min_p=None,\n",
    "    seed=42\n",
    "):\n",
    "    torch.manual_seed(seed)\n",
    "    tokens = tokenize(prompt)\n",
    "    temperature = max(temperature, 1e-8)\n",
    "    for i in range(num_tokens):\n",
    "        logits = model(tokens).logits[0, -1]\n",
    "        if top_k:\n",
    "            logits, idxs = logits.topk(top_k)\n",
    "        else:\n",
    "            idxs = torch.arange(len(logits), device=device)\n",
    "\n",
    "        # TODO: temperature before or after min_p?\n",
    "        probs = F.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "        if min_p is not None:\n",
    "            mask = probs >= (probs.max() * min_p) \n",
    "            idxs, probs = idxs[mask], probs[mask]\n",
    "\n",
    "        next_token = idxs[torch.multinomial(probs, 1)]\n",
    "        tokens = torch.cat([ tokens, next_token[None] ], dim=1)\n",
    "    return decode(tokens[0])\n",
    "\n",
    "for min_p in reversed(torch.logspace(start=math.log10(0.01), end=math.log10(0.5), steps=10, base=10)):\n",
    "    print(f\"\\n### Min P: {min_p.item():.2f} ###\")\n",
    "    print(generate(prompt, num_tokens=20, temperature=1.5, min_p=min_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d1a612-2d08-422f-ae4b-cf9d2f1a631d",
   "metadata": {},
   "source": [
    "## Frequency Penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d0cab7-b60d-4fac-82cc-3cdc6654d2da",
   "metadata": {},
   "source": [
    "As we've seen at low temperatures, the model has a tendancy to repeat itself. For this we can apply a frequency penalty to discourage the model from predicting the same token again.\n",
    "\n",
    "higher frequency -> higher penalty. If token not in sequence, count will be 0 and no penalty applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "790dbaf8-d589-4c2e-8b52-67736cd885a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Frequency Penalty 0.0 ###\n",
      "the cat sat on a mat, and the cat sat on a mat, and the cat sat on a mat, and the cat\n",
      "\n",
      "### Frequency Penalty 0.2 ###\n",
      "the cat sat on a mat, and the cat sat on a chair.\n",
      "\n",
      "\"I'm not going to lie, I\n",
      "\n",
      "### Frequency Penalty 0.4 ###\n",
      "the cat sat on a mat, and the cat was sitting on a chair.\n",
      "\n",
      "\"I'm not sure what you're\n",
      "\n",
      "### Frequency Penalty 0.6 ###\n",
      "the cat sat on a mat, and the cat was sitting on a chair.\n",
      "\n",
      "\"I'm not sure what you're\n",
      "\n",
      "### Frequency Penalty 0.8 ###\n",
      "the cat sat on a mat, and the dog was sitting on a chair.\n",
      "\n",
      "\"I'm not sure what happened to\n",
      "\n",
      "### Frequency Penalty 1.0 ###\n",
      "the cat sat on a mat, and the dog was sitting on a chair.\n",
      "\n",
      "\"I'm not sure what happened to\n"
     ]
    }
   ],
   "source": [
    "def generate(\n",
    "    prompt,\n",
    "    num_tokens,\n",
    "    temperature=1.0,\n",
    "    top_k=None,\n",
    "    min_p=None,\n",
    "    frequency_penalty=None,\n",
    "    seed=42,\n",
    "):\n",
    "    torch.manual_seed(seed)\n",
    "    tokens = tokenize(prompt)\n",
    "    temperature = max(temperature, 1e-8)\n",
    "    for i in range(num_tokens):\n",
    "        logits = model(tokens).logits[0, -1]\n",
    "\n",
    "        if frequency_penalty:\n",
    "            *_, vocab_size = logits.shape\n",
    "            # get frequency of each of the logits in the current output\n",
    "            id_freqs = torch.bincount(tokens[0], minlength=vocab_size)\n",
    "            logits -= frequency_penalty * id_freqs\n",
    "\n",
    "        if top_k:\n",
    "            logits, idxs = logits.topk(top_k)\n",
    "        else:\n",
    "            idxs = torch.arange(len(logits), device=device)\n",
    "\n",
    "        # TODO: temperature before or after min_p?\n",
    "        probs = F.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "        if min_p is not None:\n",
    "            mask = probs >= (probs.max() * min_p) \n",
    "            idxs, probs = idxs[mask], probs[mask]\n",
    "\n",
    "        next_token = idxs[torch.multinomial(probs, 1)]\n",
    "        tokens = torch.cat([ tokens, next_token[None] ], dim=1)\n",
    "    return decode(tokens[0])\n",
    "\n",
    "for freq_penalty in torch.linspace(start=0, end=1., steps=6):\n",
    "    print(f\"\\n### Frequency Penalty {freq_penalty.item():.1f} ###\")\n",
    "    print(generate(prompt, num_tokens=20, temperature=0., frequency_penalty=freq_penalty))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e244f51-c859-49a3-ad84-4e90d69f68aa",
   "metadata": {},
   "source": [
    "It no longer repeats itself continuously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9974554e-f759-4cb0-a132-b38dc0b2628a",
   "metadata": {},
   "source": [
    "## KV Cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca89dbf-2209-43d8-9c4d-7930efbe3ffb",
   "metadata": {},
   "source": [
    "When running our autoregressive `generate` function, we currently recalculate the logit outputs of every previous token in the sequence, before discarding with\n",
    "\n",
    "`logits = model(tokens).logits[0, -1]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8d65da-0f46-45cd-8e72-d51dbb42e2bb",
   "metadata": {},
   "source": [
    "## Beam Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7816d633-1989-4542-98b3-4ecd2e46b0c9",
   "metadata": {},
   "source": [
    "This is quite an advanced topic, but essentially it allows the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
