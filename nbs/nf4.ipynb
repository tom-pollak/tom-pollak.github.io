{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c12c73af-1388-4d49-a42e-73e14b17105b",
   "metadata": {},
   "source": [
    "# NF4 Dequantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9259b6e7-c24f-4433-95df-cd5e5dcc2127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.environ['TRITON_INTERPRET'] = '1'\n",
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "torch.set_printoptions(linewidth=120)\n",
    "\n",
    "def _b(*pids):\n",
    "    \"breakpoint on pids\"\n",
    "    if all(tl.program_id(i) == pid for i, pid in enumerate(pids)):\n",
    "        set_trace()\n",
    "\n",
    "def cdiv(x, y): return (x + y - 1) // y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "441f2293-313a-4eed-b040-64915fdf7669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c51698fc-7b82-4f97-9f69-8b455da4ca65",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m set_seed\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01minspect\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "# Helpful functions used through the entire notebook\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import set_seed\n",
    "import time\n",
    "import inspect\n",
    "import os\n",
    "major_version, minor_version = torch.cuda.get_device_capability()\n",
    "HAS_BFLOAT16 = (major_version >= 8)\n",
    "from inspect import currentframe as _C, getframeinfo\n",
    "_F = lambda c: getframeinfo(c).lineno # Gets line number\n",
    "WARN = lambda x: print(f\"\\033[31m{x}\\033[0m\") # Red colored warnings\n",
    "\n",
    "# https://stackoverflow.com/questions/18425225/getting-the-name-of-a-variable-as-a-string\n",
    "def NAME(var):\n",
    "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
    "    names = [var_name for var_name, var_val in callers_local_vars if var_val is var]\n",
    "    return names[0] if len(names) != 0 else \"\"\n",
    "\n",
    "def assert_same(x, y, line, dtype):\n",
    "    assert(x.dtype == dtype)\n",
    "    try: torch.testing.assert_close(x, y, check_stride = True)\n",
    "    except Exception as error:\n",
    "        raise RuntimeError(\n",
    "            f\"Failed allclose at line [{line}]: {NAME(x)}, {NAME(y)}\\n{str(error)}\"\n",
    "        )\n",
    "\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "\n",
    "# ---\n",
    "\n",
    "from bitsandbytes.nn import Linear4bit\n",
    "from transformers.activations import ACT2FN\n",
    "from peft.utils.integrations import dequantize_module_weight as peft_dequantize\n",
    "\n",
    "@torch.inference_mode\n",
    "def fast_dequantize(W, quant_state = None, out = None, use_global_buffer = False):\n",
    "    if quant_state is None: return W\n",
    "    if type(quant_state) is not list:\n",
    "        # New quant_state as a class\n",
    "        # https://github.com/TimDettmers/bitsandbytes/pull/763/files\n",
    "        absmax     = quant_state.absmax\n",
    "        shape      = quant_state.shape\n",
    "        dtype      = quant_state.dtype\n",
    "        blocksize  = quant_state.blocksize\n",
    "        offset     = quant_state.offset\n",
    "        state2     = quant_state.state2\n",
    "        absmax2    = state2.absmax\n",
    "        code2      = state2.code\n",
    "        blocksize2 = state2.blocksize\n",
    "    else:\n",
    "        # Old quant_state as a list of lists\n",
    "        absmax, shape, dtype, blocksize, compressed_stats, _, _ = quant_state\n",
    "        offset, state2 = compressed_stats\n",
    "        absmax2, code2, blocksize2, _, _, _, _ = state2\n",
    "    pass\n",
    "\n",
    "    n_elements_absmax = absmax.numel()\n",
    "\n",
    "    # Create weight matrix\n",
    "    if use_global_buffer:\n",
    "\n",
    "        # Use same buffers for faster inference\n",
    "        size = shape[0]*shape[1]\n",
    "        global WEIGHT_BUFFER\n",
    "        global ABSMAX_BUFFER\n",
    "        if WEIGHT_BUFFER is None:\n",
    "            WEIGHT_BUFFER = torch.empty(size, dtype = dtype, device = \"cuda:0\", requires_grad = False)\n",
    "            ABSMAX_BUFFER = torch.empty(n_elements_absmax, dtype = dtype, device = \"cuda:0\", requires_grad = False)\n",
    "\n",
    "        if size > WEIGHT_BUFFER.numel(): WEIGHT_BUFFER.resize_(size)\n",
    "        if n_elements_absmax > ABSMAX_BUFFER.numel(): ABSMAX_BUFFER.resize_(n_elements_absmax)\n",
    "\n",
    "        out = WEIGHT_BUFFER[:size].view(shape)\n",
    "        out_absmax = ABSMAX_BUFFER[:n_elements_absmax]\n",
    "    else:\n",
    "        if out is None:\n",
    "            out = torch.empty(shape, dtype = dtype, device = \"cuda:0\", requires_grad = False)\n",
    "        else:\n",
    "            assert(out.shape == shape)\n",
    "            assert(out.dtype == dtype)\n",
    "        out_absmax = torch.empty(n_elements_absmax, dtype = torch.float32, device = \"cuda:0\", requires_grad = False)\n",
    "    pass\n",
    "\n",
    "    # Do dequantization\n",
    "    ptr_out_absmax = get_ptr(out_absmax)\n",
    "    cdequantize_blockwise_fp32(\n",
    "        get_ptr(code2), get_ptr(absmax), get_ptr(absmax2), ptr_out_absmax,\n",
    "        ctypes_c_int(blocksize2), ctypes_c_int(n_elements_absmax),\n",
    "    )\n",
    "    out_absmax += offset\n",
    "\n",
    "    fx = cdequantize_blockwise_fp16_nf4 if dtype == torch.float16 else \\\n",
    "         cdequantize_blockwise_bf16_nf4\n",
    "    fx(get_ptr(None), get_ptr(W), ptr_out_absmax, get_ptr(out),\n",
    "       ctypes_c_int(blocksize), ctypes_c_int(out.numel()),)\n",
    "\n",
    "    # Careful returning transposed data\n",
    "    is_transposed = (True if W.shape[0] == 1 else False)\n",
    "    return out.t() if is_transposed else out\n",
    "\n",
    "def unsloth_dequantize(weight):\n",
    "    return fast_dequantize(weight.weight, weight.weight.quant_state)\n",
    "\n",
    "def bnb_Linear4bit(hd, m, dtype = torch.float16):\n",
    "    return Linear4bit(\n",
    "        hd, m, bias = None,\n",
    "        compute_dtype       = dtype,\n",
    "        compress_statistics = True,\n",
    "        quant_type          = \"nf4\",\n",
    "    )\n",
    "\n",
    "# [NEW] as at 18th Feb 2025\n",
    "def assert_correct_bnb(weight, dtype):\n",
    "    assert(weight.weight.dtype == torch.uint8)\n",
    "    assert(weight.weight.quant_state.dtype == dtype)\n",
    "    assert(weight.weight.quant_state.absmax.dtype == torch.uint8)\n",
    "    assert(weight.weight.quant_state.code.dtype == torch.float32)\n",
    "    assert(weight.weight.quant_state.offset.dtype == torch.float32)\n",
    "    assert(weight.weight.quant_state.blocksize == 64)\n",
    "    assert(weight.weight.quant_state.state2.absmax.dtype == torch.float32)\n",
    "    assert(weight.weight.quant_state.state2.code.dtype == torch.float32)\n",
    "    assert(weight.weight.quant_state.state2.blocksize == 256)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hd = 4096, m = 14336, dtype = torch.float16):\n",
    "        super().__init__()\n",
    "        self.gate_proj = bnb_Linear4bit(hd, m, dtype = dtype).to(\"cuda\")\n",
    "        self.up_proj   = bnb_Linear4bit(hd, m, dtype = dtype).to(\"cuda\")\n",
    "        self.down_proj = bnb_Linear4bit(m, hd, dtype = dtype).to(\"cuda\")\n",
    "        # [NEW] as at 18th Feb 2025\n",
    "        self.gate_proj.weight.quant_state.dtype = dtype\n",
    "        self.up_proj  .weight.quant_state.dtype = dtype\n",
    "        self.down_proj.weight.quant_state.dtype = dtype\n",
    "        self.act_fn = ACT2FN[\"silu\"]\n",
    "    def forward(self, x):\n",
    "        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "def mlp_forward(X, mlp, fx):\n",
    "    up   = X @ fx(mlp.  up_proj).t()\n",
    "    gate = X @ fx(mlp.gate_proj).t()\n",
    "    h = mlp.act_fn(gate) * up\n",
    "    down = h @ fx(mlp.down_proj).t()\n",
    "    return down\n",
    "\n",
    "def mlp_dequantize(X, mlp, fx):\n",
    "    a = fx(mlp.  up_proj).t(); torch.cuda.synchronize()\n",
    "    b = fx(mlp.gate_proj).t(); torch.cuda.synchronize()\n",
    "    c = fx(mlp.down_proj).t(); torch.cuda.synchronize()\n",
    "    return a, b, c\n",
    "\n",
    "def test_dequantize(dequantize_fx):\n",
    "    elapsed = 0\n",
    "    options = [\n",
    "        (2, 3333, 2048,  8192, 3407, torch.float16),\n",
    "        (5,  777, 1024,  4096, 3409, torch.bfloat16),\n",
    "        (3, 2048, 4096, 14336, 3408, torch.bfloat16),\n",
    "    ]\n",
    "    for (bsz, qlen, hd, m, seed, dt) in options:\n",
    "        set_seed(seed)\n",
    "        torch.set_default_dtype(torch.float32)\n",
    "        mlp = MLP(hd = hd, m = m, dtype = dt)\n",
    "        X = torch.randn((bsz, qlen, hd), device = \"cuda\", dtype = dt)\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        # Warmup\n",
    "        for _ in range(2):\n",
    "            assert_same( mlp_forward(X, mlp, dequantize_fx), mlp(X), _F(_C()), dt)\n",
    "            # [NEW] as at 18th Feb 2025\n",
    "            assert_correct_bnb(mlp.  up_proj, dt)\n",
    "            assert_correct_bnb(mlp.gate_proj, dt)\n",
    "            assert_correct_bnb(mlp.down_proj, dt)\n",
    "            a, b, c = mlp_dequantize(X, mlp, dequantize_fx)\n",
    "            A, B, C = mlp_dequantize(X, mlp, unsloth_dequantize)\n",
    "            assert_same(a, A, _F(_C()), dt)\n",
    "            assert_same(b, B, _F(_C()), dt)\n",
    "            assert_same(c, C, _F(_C()), dt)\n",
    "\n",
    "        # Benchmarking\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.time()\n",
    "        for _ in range(1000): mlp_dequantize(X, mlp, dequantize_fx)\n",
    "        elapsed += time.time() - start\n",
    "    return elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4608b333-37da-4b9b-be90-023191dd511f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b806aa-a281-42b3-a24c-4f227e474241",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def _your_dequantize_nf4_kernel():\n",
    "    ### TRITON CODE GOES HERE\n",
    "    return\n",
    "\n",
    "def _your_dequantize_nf4(weight, quant_state):\n",
    "    ### SETUP TRITON LAUNCH HERE\n",
    "    return None\n",
    "\n",
    "def your_dequantize_nf4(weight):\n",
    "    return _your_dequantize_nf4(weight.weight.data, weight.weight.quant_state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
