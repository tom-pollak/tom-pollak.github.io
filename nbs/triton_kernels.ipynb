{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17631641-dc36-4c2d-96b7-43758356f488",
   "metadata": {},
   "source": [
    "# Triton Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae702d39-e278-43eb-b4af-adf100fb5f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.environ['TRITON_INTERPRET'] = '1'\n",
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "torch.set_printoptions(linewidth=120)\n",
    "\n",
    "def _b(*pids):\n",
    "    \"breakpoint on pids\"\n",
    "    if all(tl.program_id(i) == pid for i, pid in enumerate(pids)):\n",
    "        set_trace()\n",
    "\n",
    "def cdiv(x, y): return (x + y - 1) // y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3952e212-a060-4abd-9400-239d3330a7f0",
   "metadata": {},
   "source": [
    "## How does swizzling work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f421758-9055-4ed9-87e8-d8600cd36856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzYAAAGiCAYAAAA1J1M9AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAM65JREFUeJzt3Wl4VGWa//HfqQqpBEwKEiUhmEC0aZBFRDYVVBhjc/FnUXvA0UFEdFxGFBBFQI1o25AGu21caBYvG7RbEB0FbdtlGESWEWRJ4zoiaMAIQgQ1gWAWqs7/BZ3SSEIWz0PlSb6f66oXOXW8z+2Tqtz86lTVcVzXdQUAAAAAFvNFuwEAAAAA+LkINgAAAACsR7ABAAAAYD2CDQAAAADrEWwAAAAAWI9gAwAAAMB6BBsAAAAA1iPYAAAAALAewQYAAACA9Qg2QC088MADchynXv/t4sWL5TiOdu3a5W1TP7Jr1y45jqPFixcbOwYAoOF7++235TiO3n77bU/rMmdgA4INGr2PPvpI11xzjdq2batAIKC0tDSNGjVKH330UbRbAwA0UR988IFGjBihdu3aKS4uTm3bttWll16qxx9/PNqtAdZyXNd1o90EYMpLL72kq6++WklJSbrhhhuUmZmpXbt26amnntLBgwf13HPP6YorrqixztGjR3X06FHFxcXVuYdQKKTy8nIFAoF6n/Wpya5du5SZmalFixbpuuuuM3IMAIA33nnnHQ0cOFAZGRkaM2aMUlNTlZ+fr40bN+qzzz7Tzp076107HA6rrKxMsbGx8vm8e/2aOQMbxES7AcCUzz77TKNHj9YZZ5yhtWvX6rTTTovcN2HCBF144YUaPXq03n//fZ1xxhlV1iguLlaLFi0UExOjmJj6PV38fr/8fn+9/lsAQOMzY8YMBYNBbd68WS1btqx0X0FBwc+q7fP56vUiHNAY8FY0NFoPP/ywjhw5ooULF1YKNZJ06qmnasGCBSouLtbs2bMl/fA5mo8//lj//u//rlatWql///6V7vux77//XuPHj9epp56qhIQEDR8+XHv27JHjOHrggQci+1X1GZv27dtr6NChWr9+vfr06aO4uDidccYZeuaZZyod45tvvtFdd92lbt266ZRTTlFiYqIGDx6s9957z8OVAgCcTJ999pm6dOlyXKiRpNatW0uSfv3rX+vcc8+tdN+wYcPkOI5eeeWVyLZ3331XjuPo9ddfl3T8Z2wqZlBVtwEDBkiSrrvuumr3+fE8q8onn3yiESNGKCkpSXFxcerVq1el/oCTiTM2aLT+9re/qX379rrwwgurvP+iiy5S+/bt9fe//73S9pEjR6pDhw6aOXOmTvROzeuuu07PP/+8Ro8erfPOO09r1qzRkCFDat3fzp07NWLECN1www0aM2aM/vznP+u6665Tz5491aVLF0nS559/rhUrVmjkyJHKzMzU/v37tWDBAl188cX6+OOPlZaWVuvjAQAahnbt2mnDhg368MMP1bVr1yr3ufDCC/Xyyy+rqKhIiYmJcl1X//u//yufz6d169Zp+PDhkqR169bJ5/OpX79+Vda56KKL9Je//KXStt27d+u+++6LhKibb75ZWVlZlfZ544039Oyzz0b2qcpHH32kfv36qW3btpo6dapatGih559/XpdffrlefPHFWr3VG/CUCzRC3333nSvJveyyy0643/Dhw11JblFRkTt9+nRXknv11Vcft1/FfRW2bt3qSnInTpxYab/rrrvOleROnz49sm3RokWuJDcvLy+yrV27dq4kd+3atZFtBQUFbiAQcO+8887ItpKSEjcUClU6Rl5enhsIBNzf/OY3lbZJchctWnTC/18AQPT993//t+v3+12/3++ef/757t133+2++eabbllZWWSfzZs3u5Lc1157zXVd133//fddSe7IkSPdvn37RvYbPny426NHj8jPq1evdiW5q1evrvLY33//vduzZ083LS3N/eqrr6rcZ8eOHW4wGHQvvfRS9+jRo67rVj1nLrnkErdbt25uSUlJZFs4HHYvuOACt0OHDnVeF+Dn4q1oaJQOHTokSUpISDjhfhX3FxUVRbbdcsstNdZ/4403JEm33nprpe233357rXvs3LlzpbNJp512mjp27KjPP/88si0QCEQ+/BkKhXTw4EGdcsop6tixo3Jzc2t9LABAw3HppZdqw4YNGj58uN577z3Nnj1bgwYNUtu2bSNv4+rRo4dOOeUUrV27VtKxMzOnn366rr32WuXm5urIkSNyXVfr16+v9p0JVbn11lv1wQcf6MUXX1Rqaupx9xcXF+uKK65Qq1attHTp0mo/I/rNN9/orbfe0pVXXqlDhw7pwIEDOnDggA4ePKhBgwZpx44d2rNnTz1WB6g/3oqGRqkisFQEnOpUFYAyMzNrrL979275fL7j9v3FL35R6x4zMjKO29aqVSt9++23kZ/D4bAeffRR/elPf1JeXp5CoVDkvuTk5FofCwDQsPTu3VsvvfSSysrK9N5772n58uX64x//qBEjRmjbtm3q3Lmzzj//fK1bt07SsWBz4YUXqn///gqFQtq4caNSUlL0zTff1DrYLFiwQIsWLdKCBQt03nnnVbnPjTfeqM8++0zvvPPOCefMzp075bqusrOzlZ2dXeU+BQUFatu2ba16A7xAsEGjFAwG1aZNG73//vsn3O/9999X27ZtlZiYGNkWHx9vuj1JqvZVMPdHn+uZOXOmsrOzdf311+uhhx5SUlKSfD6fJk6cqHA4fFL6BACYExsbq969e6t379765S9/qbFjx+qFF17Q9OnT1b9/f82YMUMlJSVat26d7r33XrVs2VJdu3bVunXrlJKSIkm1CjabNm3ShAkT9B//8R+66aabqtzn0Ucf1dKlS/XXv/5V55xzzgnrVcygu+66S4MGDapyn7q82Ad4gWCDRmvo0KF68skntX79+si3m/3YunXrtGvXLt188811rt2uXTuFw2Hl5eWpQ4cOke0/59oDVfmv//ovDRw4UE899VSl7d99951OPfVUT48FAIiuXr16SZK++uorSccCS1lZmZYuXao9e/ZEAsxFF10UCTa//OUvIwGnOl9//bVGjBihc845R3Pnzq1yn3Xr1umuu+7SxIkTNWrUqBp7rbhMQrNmzY774gEgWviMDRqtyZMnKz4+XjfffLMOHjxY6b5vvvlGt9xyi5o3b67JkyfXuXbFq1N/+tOfKm33+orRfr//uG9me+GFF3jfMgBYbPXq1VV+6+Zrr70mSerYsaMkqW/fvmrWrJlmzZqlpKSkyDdmXnjhhdq4caPWrFlT49maUCikq666SmVlZXrxxRcVGxt73D5fffWVrrzySvXv318PP/xwrf4fWrdurQEDBmjBggWRIPZjX3/9da3qAF7ijA0arQ4dOujpp5/WqFGj1K1bN91www3KzMzUrl279NRTT+nAgQNaunSpzjzzzDrX7tmzp/71X/9Vc+bM0cGDByNf9/zpp59K0nHXvKmvoUOH6je/+Y3Gjh2rCy64QB988IGeffbZai8oCgBo+G6//XYdOXJEV1xxhTp16qSysjK98847WrZsmdq3b6+xY8dKkpo3b66ePXtq48aNkWvYSMfO2BQXF6u4uLjGYDN//ny99dZbuuWWW7R69epK96WkpOjSSy/V+PHj9fXXX+vuu+/Wc889V2mfs88+W2effXaVtefOnav+/furW7duuvHGG3XGGWdo//792rBhg7788kuuuYaTjmCDRm3kyJHq1KmTcnJyImEmOTlZAwcO1D333FPt9QNq45lnnlFqaqqWLl2q5cuXKysrS8uWLVPHjh09u+rzPffco+LiYi1ZskTLli3Tueeeq7///e+aOnWqJ/UBACff73//e73wwgt67bXXtHDhQpWVlSkjI0O33nqr7rvvvkoX7qw4O/Pjt1SnpqbqF7/4hXbu3FljsKk4czJ//nzNnz+/0n0XX3yxLr30Un399dcKhUKaNGnScf/99OnTqw02nTt31pYtW/Tggw9q8eLFOnjwoFq3bq0ePXro/vvvr+1yAJ5x3KrOhQKol23btqlHjx7661//Wqv3KAMAAMAbfMYGqKfvv//+uG1z5syRz+fTRRddFIWOAAAAmi7eigbU0+zZs7V161YNHDhQMTExev311/X666/rpptuUnp6erTbAwAAaFJ4KxpQTytXrtSDDz6ojz/+WIcPH1ZGRoZGjx6te++9VzExvGYAAABwMhFsAAAAAFiPz9gAAAAAsB7BBgAAAID1GtwHAcLhsPbu3auEhATPLnIIAKgd13V16NAhpaWlyefjta8KzCYAiI66zKUGF2z27t3LN0oBQJTl5+fr9NNPj3YbDQazCQCiqzZzqcEFm4SEBElSf/0/xaiZt8VNvcrmmHlV0/HZ1a8M9WvsxVFTr0abatjgq+fGXoG27TFs7G+EPetwNFymNd/8JfK3GMcwm35c1ky/xYPPMVJXhpbXNTWbDNXdf56px5mZspJk7NutHDOV+/X8xEhdn6GV8BlaB58T9rReWXG5nhv6Uq3mUoMLNhX/wIpRM8U4TXx4WNavqfVlHSrqWhhsbFsLY0HMsnWQwceEpZhNPyprqN+YZnFG6hJsjvHFWRhsjK2xmX/Qx54Sa6RuUw82FWrzt4c3UAMAAACwHsEGAAAAgPUINgAAAACsR7ABAAAAYD1jwWbu3Llq37694uLi1LdvX23atMnUoQAAqBFzCQAaNyPBZtmyZZo0aZKmT5+u3Nxcde/eXYMGDVJBQYGJwwEAcELMJQBo/IwEm0ceeUQ33nijxo4dq86dO2v+/Plq3ry5/vznP5s4HAAAJ8RcAoDGz/NgU1ZWpq1btyorK+uHg/h8ysrK0oYNG47bv7S0VEVFRZVuAAB4pa5zSWI2AYCNPA82Bw4cUCgUUkpKSqXtKSkp2rdv33H75+TkKBgMRm7p6eletwQAaMLqOpckZhMA2Cjq34o2bdo0FRYWRm75+fnRbgkA0MQxmwDAPjFeFzz11FPl9/u1f//+Stv379+v1NTU4/YPBAIKBAJetwEAgKS6zyWJ2QQANvL8jE1sbKx69uypVatWRbaFw2GtWrVK559/vteHAwDghJhLANA0eH7GRpImTZqkMWPGqFevXurTp4/mzJmj4uJijR071sThAAA4IeYSADR+RoLNv/3bv+nrr7/W/fffr3379umcc87RG2+8cdwHNwEAOBmYSwDQ+BkJNpJ022236bbbbjNVHgCAOmEuAUDjFvVvRQMAAACAn4tgAwAAAMB6BBsAAAAA1jP2GZufy9/6NPl9sZ7WdBzH03o/Kmymrs9Q7rRuHSzr17a6klzW2Gxdm9Y3VCod8L5sY8FskrHZVNLS1MwzU9Y19SfZUN1wsNxMYcc1U1cGx56hnjs0LzBS1++EjdT1Garrl7frWxI+Wut9OWMDAAAAwHoEGwAAAADWI9gAAAAAsB7BBgAAAID1CDYAAAAArEewAQAAAGA9gg0AAAAA6xFsAAAAAFiPYAMAAADAegQbAAAAANYj2AAAAACwHsEGAAAAgPUINgAAAACsR7ABAAAAYD2CDQAAAADrEWwAAAAAWI9gAwAAAMB6BBsAAAAA1iPYAAAAALAewQYAAACA9Qg2AAAAAKwXE+0GqpXcUvIHPC0ZdhxP60WYioem+jVU12V9jdZ1DbUrSfJZ9pgwtRa2/e4M9Hv0qN/zmo0Ks8nY86SspW3PPzNlTfXbPPi9kbqmHr7HartW1e0cv8dIXZ/CRur6HUN15e36HikP1XpfztgAAAAAsB7BBgAAAID1CDYAAAAArEewAQAAAGA9gg0AAAAA6xFsAAAAAFiPYAMAAADAep4Hm5ycHPXu3VsJCQlq3bq1Lr/8cm3fvt3rwwAAUGvMJgBo/DwPNmvWrNG4ceO0ceNGrVy5UuXl5frVr36l4uJirw8FAECtMJsAoPGL8brgG2+8UennxYsXq3Xr1tq6dasuuugirw8HAECNmE0A0Ph5Hmx+qrCwUJKUlJRU5f2lpaUqLS2N/FxUVGS6JQBAE8dsAoDGx+iXB4TDYU2cOFH9+vVT165dq9wnJydHwWAwcktPTzfZEgCgiWM2AUDjZDTYjBs3Th9++KGee+65aveZNm2aCgsLI7f8/HyTLQEAmjhmEwA0Tsbeinbbbbfp1Vdf1dq1a3X66adXu18gEFAgEDDVBgAAEcwmAGi8PA82ruvq9ttv1/Lly/X2228rMzPT60MAAFAnzCYAaPw8Dzbjxo3TkiVL9PLLLyshIUH79u2TJAWDQcXHx3t9OAAAasRsAoDGz/PP2MybN0+FhYUaMGCA2rRpE7ktW7bM60MBAFArzCYAaPyMvBUNAICGhNkEAI2f0W9FAwAAAICTgWADAAAAwHoEGwAAAADWM3Ydm5+rPDlebkycpzVdx/G0XoShsvb1a6aufGYKG+vX0O/NWL+SsccEz41jXFMvIRno92h5yPuijQizyVy/ZUEjZQ3+rTfzuS1T/aYlHjZS12doHUzW9slM3c6x+43U9Rvq19Ro8nv8GD4UG671vpyxAQAAAGA9gg0AAAAA6xFsAAAAAFiPYAMAAADAegQbAAAAANYj2AAAAACwHsEGAAAAgPUINgAAAACsR7ABAAAAYD2CDQAAAADrEWwAAAAAWI9gAwAAAMB6BBsAAAAA1iPYAAAAALAewQYAAACA9Qg2AAAAAKxHsAEAAABgPYINAAAAAOsRbAAAAABYj2ADAAAAwHoEGwAAAADWi4l2A9UpaRWrmGax3hZ1vC1XwTVU175+zRRmff/JVF3Z17Nr6LHGY0IKlfm9L9qI+N/9RH6nmbdFfWYeII6p54nPzGuiScndjNRlNh2zu2WqmcIGZ5O5v8mukbpzEi8xUtcnM/36nLChut72W3a4XNKLtTu2p0cGAAAAgCgg2AAAAACwHsEGAAAAgPUINgAAAACsR7ABAAAAYD2CDQAAAADrEWwAAAAAWM94sPnd734nx3E0ceJE04cCAKBGzCUAaJyMBpvNmzdrwYIFOvvss00eBgCAWmEuAUDjZSzYHD58WKNGjdKTTz6pVq1amToMAAC1wlwCgMbNWLAZN26chgwZoqysrBPuV1paqqKioko3AAC8Vtu5JDGbAMBGMSaKPvfcc8rNzdXmzZtr3DcnJ0cPPvigiTYAAJBUt7kkMZsAwEaen7HJz8/XhAkT9OyzzyouLq7G/adNm6bCwsLILT8/3+uWAABNWF3nksRsAgAbeX7GZuvWrSooKNC5554b2RYKhbR27Vo98cQTKi0tld/vj9wXCAQUCAS8bgMAAEl1n0sSswkAbOR5sLnkkkv0wQcfVNo2duxYderUSVOmTDlueAAAYBJzCQCaBs+DTUJCgrp27VppW4sWLZScnHzcdgAATGMuAUDTYPwCnQAAAABgmpFvRfupt99++2QcBgCAWmEuAUDjwxkbAAAAANYj2AAAAACwHsEGAAAAgPVOymds6qM06NPRWI9zl+NtuQquobqm+mUdjrGvX1MNi8daBfpVqNTg46wR8LdOlt/n8fVtTD23fYbqGuq3tKWZ11ptev5JBvsNlhkp6ziukbrHipspa+qp8cvm+4zU9cvMGvucsJG6fnlb9/vQ0VrvyxkbAAAAANYj2AAAAACwHsEGAAAAgPUINgAAAACsR7ABAAAAYD2CDQAAAADrEWwAAAAAWI9gAwAAAMB6BBsAAAAA1iPYAAAAALAewQYAAACA9Qg2AAAAAKxHsAEAAABgPYINAAAAAOsRbAAAAABYj2ADAAAAwHoEGwAAAADWI9gAAAAAsB7BBgAAAID1CDYAAAAArEewAQAAAGC9mGg3UJ2yoCN/wPG0puttuR8Yqmtbv6zDMcb6Ncm2tbCsrk3rECrxvmZjEk4OKuwPeFrTdQw9QEy9dGmo39KWhtaB57UkKSHxeyN1fY5rpK4kOYZqm6rbObDHSF2/EzZTV2bWwedxv8Xlta/HGRsAAAAA1iPYAAAAALAewQYAAACA9Qg2AAAAAKxHsAEAAABgPYINAAAAAOsRbAAAAABYz0iw2bNnj6655holJycrPj5e3bp105YtW0wcCgCAGjGXAKDx8/wCnd9++6369eungQMH6vXXX9dpp52mHTt2qFWrVl4fCgCAGjGXAKBp8DzYzJo1S+np6Vq0aFFkW2ZmpteHAQCgVphLANA0eP5WtFdeeUW9evXSyJEj1bp1a/Xo0UNPPvlktfuXlpaqqKio0g0AAK/UdS5JzCYAsJHnwebzzz/XvHnz1KFDB7355pv6z//8T40fP15PP/10lfvn5OQoGAxGbunp6V63BABowuo6lyRmEwDYyHFd1/WyYGxsrHr16qV33nknsm38+PHavHmzNmzYcNz+paWlKi0tjfxcVFSk9PR0nTVupvyBOC9bk+t4Wu4Hhura1i/rcIyxfk2ybS0sq2vTOoRKSvTZ7+5RYWGhEhMTvT9AFNR1LknVz6Z/OXuKYvwBT/tzHUMPEFPfe2qo3z0DDT3eeF5LkvznfWukrs/x9J+RlTiGapuqm3PWciN1/U7YTF2ZWQefx/0WHwpr6Nmf12ouef5nr02bNurcuXOlbWeddZa++OKLKvcPBAJKTEysdAMAwCt1nUsSswkAbOR5sOnXr5+2b99eadunn36qdu3aeX0oAABqxFwCgKbB82Bzxx13aOPGjZo5c6Z27typJUuWaOHChRo3bpzXhwIAoEbMJQBoGjwPNr1799by5cu1dOlSde3aVQ899JDmzJmjUaNGeX0oAABqxFwCgKbB8+vYSNLQoUM1dOhQE6UBAKgz5hIANH6mvjMFAAAAAE4agg0AAAAA6xFsAAAAAFjPyGdsvFCWKPm9vT6nwYtpmbnAkW0X/+IiaGbrmrsEmuxbC8ueyzatQzhg5kJwjUVZUpzCMZZcPNpnprCpfsuCPP8kGev3jMQiI3VNXqDTVG1TdTvHmrkIqt9IVclv6GK7Xp81OdSs9nOJMzYAAAAArEewAQAAAGA9gg0AAAAA6xFsAAAAAFiPYAMAAADAegQbAAAAANYj2AAAAACwHsEGAAAAgPUINgAAAACsR7ABAAAAYD2CDQAAAADrEWwAAAAAWI9gAwAAAMB6BBsAAAAA1iPYAAAAALAewQYAAACA9Qg2AAAAAKxHsAEAAABgPYINAAAAAOsRbAAAAABYj2ADAAAAwHox0W6gOuXBsEJxYW+LOt6Wq+AaqivHNVTXTFnb6rqsr3ms8T/r2rMO4dij3hdtRGI3f6oYJ9bboj4zrzE6jqEHtM9M3aSks4zUtW/2mym7vWVbM4WZTRF/POUiI3V9hvo1Vdcvb//9Xnq4XNLfarUvZ2wAAAAAWI9gAwAAAMB6BBsAAAAA1iPYAAAAALAewQYAAACA9Qg2AAAAAKxHsAEAAABgPc+DTSgUUnZ2tjIzMxUfH68zzzxTDz30kFzX0HUcAAA4AeYSADQNnl+gc9asWZo3b56efvppdenSRVu2bNHYsWMVDAY1fvx4rw8HAMAJMZcAoGnwPNi88847uuyyyzRkyBBJUvv27bV06VJt2rTJ60MBAFAj5hIANA2evxXtggsu0KpVq/Tpp59Kkt577z2tX79egwcPrnL/0tJSFRUVVboBAOCVus4lidkEADby/IzN1KlTVVRUpE6dOsnv9ysUCmnGjBkaNWpUlfvn5OTowQcf9LoNAAAk1X0uScwmALCR52dsnn/+eT377LNasmSJcnNz9fTTT+v3v/+9nn766Sr3nzZtmgoLCyO3/Px8r1sCADRhdZ1LErMJAGzk+RmbyZMna+rUqbrqqqskSd26ddPu3buVk5OjMWPGHLd/IBBQIBDwug0AACTVfS5JzCYAsJHnZ2yOHDkin69yWb/fr3A47PWhAACoEXMJAJoGz8/YDBs2TDNmzFBGRoa6dOmif/zjH3rkkUd0/fXXe30oAABqxFwCgKbB82Dz+OOPKzs7W7feeqsKCgqUlpamm2++Wffff7/XhwIAoEbMJQBoGjwPNgkJCZozZ47mzJnjdWkAAOqMuQQATYPnn7EBAAAAgJONYAMAAADAegQbAAAAANYj2AAAAACwnudfHuCVcOJRKf6ot0Ud19t6FWUdI2WN9StD/TqW9euz7Pdmql3J5O/OruecqXWwqd9QsxLPazYmvtOS5fN5fOFOQw8Q19QfOUP9lrY0tA7GnoBmypqq6w+aeW4zm37Qsfk+I3X9MnPNLb9jpq7P436/D9U+D3DGBgAAAID1CDYAAAAArEewAQAAAGA9gg0AAAAA6xFsAAAAAFiPYAMAAADAegQbAAAAANYj2AAAAACwHsEGAAAAgPUINgAAAACsR7ABAAAAYD2CDQAAAADrEWwAAAAAWI9gAwAAAMB6BBsAAAAA1iPYAAAAALAewQYAAACA9Qg2AAAAAKxHsAEAAABgPYINAAAAAOsRbAAAAABYLybaDVQnPrFE/uaupzUdx9NyP6rrbZ/G6xqpKvlsWwdDdX2WPc4k+353xvo1UtVcvybqHvWXaqfnVRuPUHKCHH+cpzVdcw88I2VdQ8O0tKWphTDEULumHg+tEo8Yqcts+kGXwJdG6vplaoaEjdT1ut/DZbXvkzM2AAAAAKxHsAEAAABgPYINAAAAAOsRbAAAAABYj2ADAAAAwHoEGwAAAADWq3OwWbt2rYYNG6a0tDQ5jqMVK1ZUut91Xd1///1q06aN4uPjlZWVpR07dnjVLwAAlTCXAABSPYJNcXGxunfvrrlz51Z5/+zZs/XYY49p/vz5evfdd9WiRQsNGjRIJSUlP7tZAAB+irkEAJDqcYHOwYMHa/DgwVXe57qu5syZo/vuu0+XXXaZJOmZZ55RSkqKVqxYoauuuurndQsAwE8wlwAAksefscnLy9O+ffuUlZUV2RYMBtW3b19t2LChyv+mtLRURUVFlW4AAHihPnNJYjYBgI08DTb79u2TJKWkpFTanpKSErnvp3JychQMBiO39PR0L1sCADRh9ZlLErMJAGwU9W9FmzZtmgoLCyO3/Pz8aLcEAGjimE0AYB9Pg01qaqokaf/+/ZW279+/P3LfTwUCASUmJla6AQDghfrMJYnZBAA28jTYZGZmKjU1VatWrYpsKyoq0rvvvqvzzz/fy0MBAFAj5hIANB11/la0w4cPa+fOnZGf8/LytG3bNiUlJSkjI0MTJ07Ub3/7W3Xo0EGZmZnKzs5WWlqaLr/8ci/7BgBAEnMJAHBMnYPNli1bNHDgwMjPkyZNkiSNGTNGixcv1t13363i4mLddNNN+u6779S/f3+98cYbiouL865rAAD+ibkEAJAkx3VdN9pN/FhRUZGCwaA6/HWq/M0DntZ2HE/L/aiumSU0VtdIVcln2zoYquuz7HEm2fe7M9avkarm+jVR92hxqTZc/rgKCwv5XMmPVMymgT2nKcbvbSByzT3wjJR1DQ3TPQNaGKlrjKHfm6nHQ4vzDxipy2z6we87vmCkrl+mZkjYSF2v+z18KKx/6fZlreZS1L8VDQAAAAB+LoINAAAAAOsRbAAAAABYj2ADAAAAwHp1/la0k+W0hMOKaVHuaU2bPsArST5DHxaz7QPd1tW17EN+x2pbtsbW/e7seUyU+8q0wfOqjUdpqziFmlny5QGW1S0PmnmeGPtou2Xr2zah0EhdZtMPOjcrMVLXZ+gLO/yGHmw+j8+bFDWry7EBAAAAwHIEGwAAAADWI9gAAAAAsB7BBgAAAID1CDYAAAAArEewAQAAAGA9gg0AAAAA6xFsAAAAAFiPYAMAAADAegQbAAAAANYj2AAAAACwHsEGAAAAgPUINgAAAACsR7ABAAAAYD2CDQAAAADrEWwAAAAAWI9gAwAAAMB6BBsAAAAA1iPYAAAAALAewQYAAACA9Qg2AAAAAKwXE+0GqpNxyreKPSXW05o+uZ7Wi9R1TNUNG6nrN9WvsfU1sw6mfm9+2dWvydq2rYXf1GPN0HPDRL8lbrnnNRuT+K2fK8bxdjbJ53hbr4Jjqq6Z10RbJf3CSF0ZWgbXUF1T/b7XMsNMYVPrIEmm5p6hnv9wSl8jdZv6bCo5XC7py1rtyxkbAAAAANYj2AAAAACwHsEGAAAAgPUINgAAAACsR7ABAAAAYD2CDQAAAADr1TnYrF27VsOGDVNaWpocx9GKFSsi95WXl2vKlCnq1q2bWrRoobS0NF177bXau3evlz0DABDBXAIASPUINsXFxerevbvmzp173H1HjhxRbm6usrOzlZubq5deeknbt2/X8OHDPWkWAICfYi4BAKR6XKBz8ODBGjx4cJX3BYNBrVy5stK2J554Qn369NEXX3yhjAxDF4cCADRZzCUAgFSPYFNXhYWFchxHLVu2rPL+0tJSlZaWRn4uKioy3RIAoAmraS5JzCYAsJHRLw8oKSnRlClTdPXVVysxMbHKfXJychQMBiO39PR0ky0BAJqw2swlidkEADYyFmzKy8t15ZVXynVdzZs3r9r9pk2bpsLCwsgtPz/fVEsAgCastnNJYjYBgI2MvBWtYnjs3r1bb7311glfFQsEAgoEAibaAABAUt3mksRsAgAbeR5sKobHjh07tHr1aiUnJ3t9CAAAao25BABNQ52DzeHDh7Vz587Iz3l5edq2bZuSkpLUpk0bjRgxQrm5uXr11VcVCoW0b98+SVJSUpJiY2O96xwAADGXAADH1DnYbNmyRQMHDoz8PGnSJEnSmDFj9MADD+iVV16RJJ1zzjmV/rvVq1drwIAB9e8UAIAqMJcAAFI9gs2AAQPkum6195/oPgAAvMZcAgBIhr/uGQAAAABOBoINAAAAAOsRbAAAAABYj2ADAAAAwHpGLtDphTObf6245s08rel3wp7Wq+AzVNcvMx94pd9j/DL1eDCzDqb6lQw+Nwz17LdsjW16zh0pD3les1FJbiX5Pb5wp+N4W6+Cz1BdQ/2WtjTVr5myrqG6pvqNDZYaqesY+nt8rLapumZ6Pit+r5G6TX021WUuccYGAAAAgPUINgAAAACsR7ABAAAAYD2CDQAAAADrEWwAAAAAWI9gAwAAAMB6BBsAAAAA1iPYAAAAALAewQYAAACA9Qg2AAAAAKxHsAEAAABgPYINAAAAAOsRbAAAAABYj2ADAAAAwHoEGwAAAADWI9gAAAAAsB7BBgAAAID1CDYAAAAArEewAQAAAGA9gg0AAAAA6xFsAAAAAFgvJtoNVKdT3FdqHu/3tKZPYU/rVfA7hurKNVKXdTjGZ6xfu9ZBMrkWZno295iwrF8DdQ/HmnksNBZHk1tIMXHeFnUcb+v9k2umrLF+y4JGykqG1sHc+pope2rwsJG6ppZBMvc32VTdLrF7jdRt6rOpLnOJMzYAAAAArEewAQAAAGA9gg0AAAAA6xFsAAAAAFiPYAMAAADAegQbAAAAANarc7BZu3athg0bprS0NDmOoxUrVlS77y233CLHcTRnzpyf0SIAANVjLgEApHoEm+LiYnXv3l1z58494X7Lly/Xxo0blZaWVu/mAACoCXMJACDV4wKdgwcP1uDBg0+4z549e3T77bfrzTff1JAhQ+rdHAAANWEuAQCkegSbmoTDYY0ePVqTJ09Wly5daty/tLRUpaWlkZ+Lioq8bgkA0ITVdS5JzCYAsJHnXx4wa9YsxcTEaPz48bXaPycnR8FgMHJLT0/3uiUAQBNW17kkMZsAwEaeBputW7fq0Ucf1eLFi+U4Tq3+m2nTpqmwsDByy8/P97IlAEATVp+5JDGbAMBGngabdevWqaCgQBkZGYqJiVFMTIx2796tO++8U+3bt6/yvwkEAkpMTKx0AwDAC/WZSxKzCQBs5OlnbEaPHq2srKxK2wYNGqTRo0dr7NixXh4KAIAaMZcAoOmoc7A5fPiwdu7cGfk5Ly9P27ZtU1JSkjIyMpScnFxp/2bNmik1NVUdO3b8+d0CAPATzCUAgFSPYLNlyxYNHDgw8vOkSZMkSWPGjNHixYs9awwAgNpgLgEApHoEmwEDBsh13Vrvv2vXrroeAgCAWmMuAQAkA1/3DAAAAAAnG8EGAAAAgPUINgAAAACsR7ABAAAAYD1Pr2PjpU6xBTol1tvc5VftP1xaF6bSob/2F8muE2P9mqpbh6uF14W5dTDUr6F1kAz2bGiV/Y6Zuqb69RlaXxPrUBQb9rxmY1LaKqBQs4C3RQ09tV1Tf+QM9VsWNDOj5Ziqa6asa6huesJ3Rur6DP3bSpJ8hn53PsfM37mzPP53a4WmPpvqMpc4YwMAAADAegQbAAAAANYj2AAAAACwHsEGAAAAgPUINgAAAACsR7ABAAAAYD2CDQAAAADrEWwAAAAAWI9gAwAAAMB6BBsAAAAA1iPYAAAAALAewQYAAACA9Qg2AAAAAKxHsAEAAABgPYINAAAAAOsRbAAAAABYj2ADAAAAwHoEGwAAAADWI9gAAAAAsB7BBgAAAID1YqLdwE+5ritJOnw47Hltv1zPa0qSY6Sq5DdU2FSa9Zuq65hZCHPrYKZfx9A6SOZ6NrbGlj0mfIbW18TfiKJ//u2t+FuMYyrW4+jREu+LG3pqu6Ye0Ib6DZcYmiKOoceyqd+bobrlxWVG6voM/dtKknyGfnc+x/t/Y0pS0SEzdZv6bKrLXHLcBja9vvzyS6Wnp0e7DQBo0vLz83X66adHu40Gg9kEANFVm7nU4IJNOBzW3r17lZCQUOMr1EVFRUpPT1d+fr4SExNPUof1R79m0a95tvVMv3Xnuq4OHTqktLQ0+Xy8W7kCs6nhoF+z6Ncs+q27usylBvdWNJ/PV+dXCRMTE614cFSgX7Po1zzbeqbfugkGg1E7dkPFbGp46Ncs+jWLfuumtnOJl+MAAAAAWI9gAwAAAMB6VgebQCCg6dOnKxAIRLuVWqFfs+jXPNt6pl9Eg22/R/o1i37Nol+zbOu3wX15AAAAAADUldVnbAAAAABAItgAAAAAaAQINgAAAACsR7ABAAAAYD2rg83cuXPVvn17xcXFqW/fvtq0aVO0W6pSTk6OevfurYSEBLVu3VqXX365tm/fHu22au13v/udHMfRxIkTo91Ktfbs2aNrrrlGycnJio+PV7du3bRly5Zot1WlUCik7OxsZWZmKj4+XmeeeaYeeughNZTv8Vi7dq2GDRumtLQ0OY6jFStWVLrfdV3df//9atOmjeLj45WVlaUdO3ZEp1mduN/y8nJNmTJF3bp1U4sWLZSWlqZrr71We/fubZD9/tQtt9wix3E0Z86ck9Yffj5mk3k2zCWJ2eQlZlP0+v2phjybrA02y5Yt06RJkzR9+nTl5uaqe/fuGjRokAoKCqLd2nHWrFmjcePGaePGjVq5cqXKy8v1q1/9SsXFxdFurUabN2/WggULdPbZZ0e7lWp9++236tevn5o1a6bXX39dH3/8sf7whz+oVatW0W6tSrNmzdK8efP0xBNP6P/+7/80a9YszZ49W48//ni0W5MkFRcXq3v37po7d26V98+ePVuPPfaY5s+fr3fffVctWrTQoEGDVFJScpI7PeZE/R45ckS5ubnKzs5Wbm6uXnrpJW3fvl3Dhw+PQqfH1LS+FZYvX66NGzcqLS3tJHUGLzCbzLNhLknMJq8xm8xqNLPJtVSfPn3ccePGRX4OhUJuWlqam5OTE8WuaqegoMCV5K5ZsybarZzQoUOH3A4dOrgrV650L774YnfChAnRbqlKU6ZMcfv37x/tNmptyJAh7vXXX19p269//Wt31KhRUeqoepLc5cuXR34Oh8Nuamqq+/DDD0e2fffdd24gEHCXLl0ahQ4r+2m/Vdm0aZMryd29e/fJaeoEquv3yy+/dNu2bet++OGHbrt27dw//vGPJ7031A+zySxb5pLrMptMYjaZZfNssvKMTVlZmbZu3aqsrKzINp/Pp6ysLG3YsCGKndVOYWGhJCkpKSnKnZzYuHHjNGTIkErr3BC98sor6tWrl0aOHKnWrVurR48eevLJJ6PdVrUuuOACrVq1Sp9++qkk6b333tP69es1ePDgKHdWs7y8PO3bt6/SYyIYDKpv375WPPekY88/x3HUsmXLaLdSpXA4rNGjR2vy5Mnq0qVLtNtBHTCbzLNlLknMppOJ2WSeLbMpJtoN1MeBAwcUCoWUkpJSaXtKSoo++eSTKHVVO+FwWBMnTlS/fv3UtWvXaLdTreeee065ubnavHlztFup0eeff6558+Zp0qRJuueee7R582aNHz9esbGxGjNmTLTbO87UqVNVVFSkTp06ye/3KxQKacaMGRo1alS0W6vRvn37JKnK517FfQ1ZSUmJpkyZoquvvlqJiYnRbqdKs2bNUkxMjMaPHx/tVlBHzCazbJpLErPpZGI2mWfLbLIy2Nhs3Lhx+vDDD7V+/fpot1Kt/Px8TZgwQStXrlRcXFy026lROBxWr169NHPmTElSjx499OGHH2r+/PkNcng8//zzevbZZ7VkyRJ16dJF27Zt08SJE5WWltYg+20sysvLdeWVV8p1Xc2bNy/a7VRp69atevTRR5WbmyvHcaLdDpqQhj6bbJtLErMJtcNs8paVb0U79dRT5ff7tX///krb9+/fr9TU1Ch1VbPbbrtNr776qlavXq3TTz892u1Ua+vWrSooKNC5556rmJgYxcTEaM2aNXrssccUExOjUCgU7RYradOmjTp37lxp21lnnaUvvvgiSh2d2OTJkzV16lRdddVV6tatm0aPHq077rhDOTk50W6tRhXPL9ueexWDY/fu3Vq5cmWDfUVs3bp1KigoUEZGRuS5t3v3bt15551q3759tNtDDZhN5tg2lyRm08nEbDLLptlkZbCJjY1Vz549tWrVqsi2cDisVatW6fzzz49iZ1VzXVe33Xabli9frrfeekuZmZnRbumELrnkEn3wwQfatm1b5NarVy+NGjVK27Ztk9/vj3aLlfTr1++4ryj99NNP1a5duyh1dGJHjhyRz1f5qef3+xUOh6PUUe1lZmYqNTW10nOvqKhI7777boN87kk/DI4dO3bof/7nf5ScnBztlqo1evRovf/++5Wee2lpaZo8ebLefPPNaLeHGjCbzLFtLknMppOJ2WSWTbPJ2reiTZo0SWPGjFGvXr3Up08fzZkzR8XFxRo7dmy0WzvOuHHjtGTJEr388stKSEiIvN8zGAwqPj4+yt0dLyEh4bj3WLdo0ULJyckN8r3Xd9xxhy644ALNnDlTV155pTZt2qSFCxdq4cKF0W6tSsOGDdOMGTOUkZGhLl266B//+IceeeQRXX/99dFuTZJ0+PBh7dy5M/JzXl6etm3bpqSkJGVkZGjixIn67W9/qw4dOigzM1PZ2dlKS0vT5Zdf3uD6bdOmjUaMGKHc3Fy9+uqrCoVCkedfUlKSYmNjG1S/GRkZxw23Zs2aKTU1VR07djzZraIemE1m2DaXJGaT15hN0evXqtkU3S9l+3kef/xxNyMjw42NjXX79Onjbty4MdotVUlSlbdFixZFu7Vaa+hfq/m3v/3N7dq1qxsIBNxOnTq5CxcujHZL1SoqKnInTJjgZmRkuHFxce4ZZ5zh3nvvvW5paWm0W3Nd13VXr15d5eN1zJgxruse+1rN7OxsNyUlxQ0EAu4ll1zibt++vUH2m5eXV+3zb/Xq1Q2u36o01K/URPWYTSdHQ59Lrsts8hKzKXr9VqWhzibHdRvIJWUBAAAAoJ6s/IwNAAAAAPwYwQYAAACA9Qg2AAAAAKxHsAEAAABgPYINAAAAAOsRbAAAAABYj2ADAAAAwHoEGwAAAADWI9gAAAAAsB7BBgAAAID1CDYAAAAArEewAQAAAGC9/w9TY1fGQwmw5AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@triton.jit\n",
    "def orig(x_ptr, n0, n1, b0: tl.constexpr, b1: tl.constexpr):\n",
    "    pid_0, pid_1 = tl.program_id(0), tl.program_id(1)\n",
    "    off_0, off_1 = pid_0 * b0 + tl.arange(0, b0), pid_1 * b1 + tl.arange(0, b1)\n",
    "    off_x = off_0[:, None] * n1 + off_1[None, :]\n",
    "    tl.store(x_ptr + off_x, off_x)\n",
    "    \n",
    "@triton.jit\n",
    "def swiz(x_ptr, n0, n1, b0: tl.constexpr, b1: tl.constexpr):\n",
    "    pid_0, pid_1 = tl.program_id(0), tl.program_id(1)\n",
    "    # original pids (used for color)\n",
    "    orig_off_0, orig_off_1 = pid_0 * b0 + tl.arange(0, b0), pid_1 * b1 + tl.arange(0, b1)\n",
    "    data = orig_off_0[:, None] * n1 + orig_off_1[None, :]\n",
    "    # swizzle!\n",
    "    # tile width: b0\n",
    "    # we can then use pid_0 and pid_1 how we normally would\n",
    "    num_0, num_1 = tl.num_programs(0), tl.num_programs(1)\n",
    "    pid_0, pid_1 = tl.swizzle2d(pid_0, pid_1, num_0, num_1, b0)\n",
    "    ###\n",
    "    off_0, off_1 = pid_0 * b0 + tl.arange(0, b0), pid_1 * b1 + tl.arange(0, b1)\n",
    "    msk_0, msk_1 = off_0 < n0, off_1 < n1\n",
    "    \n",
    "    off_x = off_0[:, None] * n1 + off_1[None, :]\n",
    "    msk_x = msk_0[:, None] & msk_1[None, :]\n",
    "    tl.store(x_ptr + off_x, data, msk_x)\n",
    "\n",
    "n, m, b0, b1 = 16, 16, 4, 4\n",
    "\n",
    "orig_out = torch.empty(n, m)\n",
    "orig[(cdiv(n, b0), cdiv(m, b1))](orig_out, n, m, b0, b1)\n",
    "\n",
    "swiz_out = torch.empty(n, m)\n",
    "swiz[(cdiv(n, b0), cdiv(m, b1))](swiz_out, n, m, b0, b1)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 6))\n",
    "ax1.imshow(orig_out)\n",
    "ax2.imshow(swiz_out)\n",
    "ax1.set_title(\"Original\")\n",
    "ax2.set_title(\"Swizzle\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724bb600-26e3-449f-a32a-1317ba12aa00",
   "metadata": {},
   "source": [
    "## Tiled Matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb4ae605-79e3-4c5f-ac2d-ce9a8555213a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== pytorch ===\n",
      "tensor([[[  70.,   76.,   82.,   88.,   94.],\n",
      "         [ 190.,  212.,  234.,  256.,  278.],\n",
      "         [ 310.,  348.,  386.,  424.,  462.]],\n",
      "\n",
      "        [[1510., 1564., 1618., 1672., 1726.],\n",
      "         [1950., 2020., 2090., 2160., 2230.],\n",
      "         [2390., 2476., 2562., 2648., 2734.]]])\n",
      "\n",
      "=== matmul_k ===\n",
      "tensor([[[  70.,   76.,   82.,   88.,   94.],\n",
      "         [ 190.,  212.,  234.,  256.,  278.],\n",
      "         [ 310.,  348.,  386.,  424.,  462.]],\n",
      "\n",
      "        [[1510., 1564., 1618., 1672., 1726.],\n",
      "         [1950., 2020., 2090., 2160., 2230.],\n",
      "         [2390., 2476., 2562., 2648., 2734.]]])\n",
      "Passed!\n"
     ]
    }
   ],
   "source": [
    "def matmul(x, y, tile_width=4, batch_width=1):\n",
    "    b, m,  k = x.shape\n",
    "    b2, k2, n = y.shape\n",
    "    assert b == b2 and k == k2\n",
    "    assert x.dtype == y.dtype and x.device == y.device\n",
    "    z = torch.empty(b, m, n, dtype=x.dtype, device=x.device)\n",
    "    matmul_k[(cdiv(m, tile_width), cdiv(n, tile_width), cdiv(b, batch_width))](\n",
    "        x, y, z,\n",
    "        b, m, n, k,\n",
    "        bw=batch_width, tw=tile_width,\n",
    "    )\n",
    "    return z\n",
    "\n",
    "@triton.jit\n",
    "def batchify(off_i, msk_i, off_b, msk_b, batch_stride):\n",
    "    off_ib = off_b[:, None, None] * batch_stride + off_i[None, :, :]\n",
    "    msk_ib = msk_b[:, None, None] & msk_i[None, :, :]\n",
    "    return off_ib, msk_ib\n",
    "\n",
    "@triton.jit\n",
    "def matmul_k(\n",
    "    x_ptr, y_ptr, z_ptr,\n",
    "    b, m, n, k,\n",
    "    bw: tl.constexpr, tw: tl.constexpr\n",
    "):\n",
    "    b, m, n, k = tl.multiple_of(b, bw), tl.multiple_of(m, tw), tl.multiple_of(n, tw), tl.multiple_of(k, tw)\n",
    "    # m,n\n",
    "    pid_m, pid_n, pid_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)\n",
    "    num_m, num_n = tl.num_programs(0), tl.num_programs(1)\n",
    "    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_m, num_n, tw) # memory coalesce\n",
    "    off_m, off_n = pid_m * tw + tl.arange(0, tw), pid_n * tw + tl.arange(0, tw)\n",
    "    # b\n",
    "    off_b = pid_b * bw + tl.arange(0, bw)\n",
    "    msk_b = off_b < b\n",
    "\n",
    "    total = tl.zeros((bw, tw, tw), dtype=tl.float32)\n",
    "    for ki in tl.range(tl.cdiv(k, tw), num_stages=2, loop_unroll_factor=4):\n",
    "        off_k = ki * tw + tl.arange(0, tw)\n",
    "\n",
    "        # x: (m, k)\n",
    "        # - stride by k\n",
    "        # - tile down dim1\n",
    "        off_x = off_m[:, None] * k + off_k[None, :]\n",
    "        msk_x = (off_m < m)[:, None] & (off_k < k)[None, :]\n",
    "        off_xb, msk_xb = batchify(off_x, msk_x, off_b, msk_b, batch_stride=m*k)\n",
    "        x = tl.load(x_ptr + off_xb, msk_xb)\n",
    "    \n",
    "        # y: (k, n)\n",
    "        # - stride by n\n",
    "        # - tile down dim0\n",
    "        off_y = off_k[:, None] * n + off_n[None, :]\n",
    "        msk_y = (off_k < k)[:, None] & (off_n < n)[None, :]\n",
    "        off_yb, msk_yb = batchify(off_y, msk_y, off_b, msk_b, batch_stride=k*n)\n",
    "        y = tl.load(y_ptr + off_yb, msk_yb)\n",
    "\n",
    "        total += tl.dot(x, y)\n",
    "\n",
    "    # save tile at pid_b, pid_m, pid_n\n",
    "    off_z = off_m[:, None] * n + off_n[None, :]\n",
    "    msk_z = (off_m[:, None] < m) & (off_n[None, :] < n)\n",
    "    off_zb, msk_zb = batchify(off_z, msk_z, off_b, msk_b, batch_stride=m*n)\n",
    "    tl.store(z_ptr + off_zb, total, msk_zb)\n",
    "\n",
    "x = torch.arange(24).reshape(2, 3, 4).float()\n",
    "y = torch.arange(40).reshape(2, 4, 5).float()\n",
    "z_gt = torch.einsum('bmk,bkn->bmn', x, y)\n",
    "z = matmul(x, y, tile_width=2, batch_width=2)\n",
    "print(\"=== pytorch ===\")\n",
    "print(z_gt)\n",
    "print(\"\\n=== matmul_k ===\")\n",
    "print(z)\n",
    "torch.testing.assert_close(z, z_gt)\n",
    "print(\"Passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418cf919-ecd3-455f-a639-c625be9cf395",
   "metadata": {},
   "source": [
    "## Quantized Tiled Matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f92f0a79-ac89-4af2-a151-1357c1cf0bc8",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'(' was never closed (965855586.py, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 22\u001b[0;36m\u001b[0m\n\u001b[0;31m    scale = tl.load(scale_ptr + off_0[:, None] * 8 + off_8[None, :],\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '(' was never closed\n"
     ]
    }
   ],
   "source": [
    "FPINT = 32 // 4\n",
    "GROUP = 8\n",
    "\n",
    "@triton.jit\n",
    "def extract(x):\n",
    "    over = tl.arange(0, 8) * 4\n",
    "    mask = 2**4 - 1\n",
    "    return (tl.expand_dims(x, -1) >> over) & mask\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def quant_matmul_k(\n",
    "    scale_ptr, offset_ptr, weight_ptr, activation_ptr, z_ptr,\n",
    "    b, m, n, k,\n",
    "    bw: tl.constexpr, tw: tl.constexpr\n",
    "):\n",
    "    pid_0, pid_1 = tl.program_id(0), tl.program_id(1)\n",
    "    off_0, off_1 = pid_0 * m + tl.arange(0, m), pid_1 * n + tl.arange(0, n)\n",
    "\n",
    "    off_8 = tl.arange(0, 8)\n",
    "\n",
    "    scale = tl.load(scale_ptr + off_0[:, None] * 8 + off_8[None, :], \n",
    "    \n",
    "def quant_matmul(\n",
    "    scale,\n",
    "    offset,\n",
    "    weight,\n",
    "    activation,\n",
    "    tile_width=4,\n",
    "    batch_width=1,\n",
    "):\n",
    "\n",
    "    b, m, f = scale.shape\n",
    "    b2, k, n = activation.shape\n",
    "    assert f == FPINT and b == b2\n",
    "    assert offset.shape == (b, m) and weight.shape == (b, m, f)\n",
    "    z = torch.zeros(b, m, n)\n",
    "    quant_matmul_k[(cdiv(m, tile_width), cdiv(n, tile_width), cdiv(b, batch_width))](\n",
    "        scale, offset, weight, activation, z,\n",
    "        b, m, n, k,\n",
    "        bw=batch_width, tw=tile_width,\n",
    "    )\n",
    "    return z\n",
    "\n",
    "def quant_matmul_gt(\n",
    "    scale     : Float32[Tensor, \"32 8\" ],\n",
    "    offset    : Int32[  Tensor, \"32\"   ],\n",
    "    weight    : Int32[  Tensor, \"32 8\" ],\n",
    "    activation: Float32[Tensor, \"64 32\"],\n",
    ") -> Float32[Tensor, \"32 32\"]:\n",
    "    offset = offset.view(32, 1)\n",
    "    def extract(x):\n",
    "        over = torch.arange(8) * 4\n",
    "        mask = 2**4 - 1\n",
    "        return (x[..., None] >> over) & mask\n",
    "    scale = scale[..., None].expand(-1, 8, GROUP).contiguous().view(-1, 64)\n",
    "    offset = extract(offset)[..., None].expand(-1, 1, 8, GROUP).contiguous().view(-1, 64)\n",
    "    return ( scale * (extract(weight).view(-1, 64) - offset))  @ activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828be6a9-2baa-479a-a8fc-abe30bb342d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = torch.arange(32 * 8).reshape("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a984cf8c-4993-4e91-adf1-1b6480538c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8, 7, 6, 5, 4, 3, 2, 1],\n",
      "        [1, 3, 2, 4, 3, 5, 3, 4]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([0x12345678, 0x43534231], dtype=torch.int32)  # Just one value\n",
    "over = torch.arange(8) * 4\n",
    "mask = 2**4 - 1\n",
    "result = (a[..., None] >> over) & mask\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fac5b128-3498-4493-9cb9-3ecc18e581d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8, 7, 6, 5, 4, 3, 2, 1],\n",
      "        [1, 3, 2, 4, 3, 5, 3, 4]], dtype=torch.int32)\n",
      "tensor([[8, 7, 6, 5, 4, 3, 2, 1],\n",
      "        [1, 3, 2, 4, 3, 5, 3, 4]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_gt(x):\n",
    "    over = torch.arange(8) * 4\n",
    "    mask = 2**4 - 1\n",
    "    return (x[..., None] >> over) & mask\n",
    "\n",
    "@triton.jit\n",
    "def test_extract(x_ptr, z_ptr):\n",
    "    x = tl.load(x_ptr + tl.arange(0, 2))\n",
    "    ex_x = extract(x)\n",
    "    tl.store(z_ptr + tl.arange(0, 8 * 2).reshape(2, 8), ex_x)\n",
    "    \n",
    "\n",
    "x = torch.tensor([0x12345678, 0x43534231], dtype=torch.int32)\n",
    "extract_gt(x)\n",
    "\n",
    "z = torch.zeros(2, 8, dtype=torch.int32)\n",
    "test_extract[(1,)](x, z)\n",
    "print(z)\n",
    "print(extract_gt(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111c52e1-451c-4513-84c6-8b5b017c77dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45835b8-17e0-4348-8235-a516d5e303a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1699fc8-e5c9-4d88-99ff-699a4d2615e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5355bf4-d7a6-4e8b-8256-54efbdf76929",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbbadb2-9936-4af9-b543-19103cf1dcc1",
   "metadata": {},
   "source": [
    "### (Simple) Vectorized Softmax\n",
    "\n",
    "This loads an entire row at once, predicated that the row is no longer than `max_t_width`. This makes the code much clearer and more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47496111-65b3-4d08-b6a8-27b1fbe2bc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== pytorch ===\n",
      "tensor([[0.0202, 0.1261, 0.0310, 0.0108, 0.0319, 0.0405, 0.0212, 0.0103],\n",
      "        [0.0520, 0.1045, 0.0138, 0.0075, 0.0269, 0.0037, 0.0063, 0.0071]])\n",
      "\n",
      "=== softmax_k ===\n",
      "tensor([[0.0202, 0.1261, 0.0310, 0.0108, 0.0319, 0.0405, 0.0212, 0.0103],\n",
      "        [0.0520, 0.1045, 0.0138, 0.0075, 0.0269, 0.0037, 0.0063, 0.0071]])\n",
      "Passed!\n"
     ]
    }
   ],
   "source": [
    "@triton.jit\n",
    "def softmax_k(x_ptr, z_ptr, n0, n1, b0: tl.constexpr, b1: tl.constexpr):\n",
    "    tl.static_assert(n1 <= b1, \"sequence length exceeds max block size\")\n",
    "    log2_e = 1.44269504\n",
    "    pid_0 = tl.program_id(0)\n",
    "    off_0 = pid_0 * b0 + tl.arange(0, b0)\n",
    "    msk_0 = off_0 < n0\n",
    "\n",
    "    off_1 = tl.arange(0, b1)\n",
    "    msk_1 = off_1 < n1\n",
    "\n",
    "    off = off_0[:, None] * n1 + off_1[None, :]\n",
    "    msk = msk_0[:, None] & msk_1[None, :]\n",
    "\n",
    "    x = tl.load(x_ptr + off, msk, -float('inf'))\n",
    "    exp_x = tl.exp2(log2_e * (x - tl.max(x, axis=1, keep_dims=True))) \n",
    "    z = exp_x / tl.sum(exp_x, axis=1, keep_dims=True)\n",
    "    tl.store(z_ptr + off, z, msk)\n",
    "    \n",
    "def softmax(x, batch_width=2, max_t_width=2048):\n",
    "    b, t = x.shape\n",
    "    z = torch.empty(b, t, device=x.device, dtype=x.dtype)\n",
    "    softmax_k[(cdiv(b, batch_width),)](\n",
    "        x, z,\n",
    "        b, t,\n",
    "        b0=batch_width, b1=max_t_width\n",
    "    )\n",
    "    return z\n",
    "\n",
    "\n",
    "def softmax_gt(x):\n",
    "    b, t = x.shape\n",
    "    x_max = x.max(1, keepdim=True)[0]\n",
    "    x = x - x_max\n",
    "    x_exp = x.exp()\n",
    "    return x_exp / x_exp.sum(1, keepdim=True)\n",
    "\n",
    "x = torch.randn(4, 32).float()\n",
    "z_gt = softmax_gt(x)\n",
    "z = softmax(x)\n",
    "print(\"=== pytorch ===\")\n",
    "print(z_gt[:2, :8])\n",
    "print(\"\\n=== softmax_k ===\")\n",
    "print(z[:2, :8])\n",
    "torch.testing.assert_close(z, z_gt)\n",
    "print(\"Passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5471baf7-db2e-46d8-b6d8-8453fe783574",
   "metadata": {},
   "source": [
    "### Naive Softmax\n",
    "\n",
    "Softmax closer to a cuda version. This allows the row width to be arbitaritly large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71e4524b-d2b4-42bb-a22d-28b41ef1cf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== pytorch ===\n",
      "tensor([[0.0129, 0.0252, 0.0320, 0.0426, 0.0252, 0.0781, 0.1193, 0.0392],\n",
      "        [0.0034, 0.1132, 0.1172, 0.3084, 0.0563, 0.0414, 0.0152, 0.0136]])\n",
      "\n",
      "=== softmax_k ===\n",
      "tensor([[0.0129, 0.0252, 0.0320, 0.0426, 0.0252, 0.0781, 0.1193, 0.0392],\n",
      "        [0.0034, 0.1132, 0.1172, 0.3084, 0.0563, 0.0414, 0.0152, 0.0136]])\n",
      "Passed!\n"
     ]
    }
   ],
   "source": [
    "@triton.jit\n",
    "def softmax_k(x_ptr, z_ptr, n0, n1, b0: tl.constexpr, b1: tl.constexpr):\n",
    "    log2_e = 1.44269504\n",
    "    pid_0 = tl.program_id(0)\n",
    "    off_0 = pid_0 * b0 + tl.arange(0, b0)\n",
    "    msk_0 = off_0 < n0\n",
    "\n",
    "    _max = tl.full((b0, 1), -float('inf'), dtype=tl.float32)\n",
    "    for i_1 in tl.range(tl.cdiv(n1, b1)):\n",
    "        off_1 = i_1 * b1 + tl.arange(0, b1)\n",
    "        msk_1 = off_1 < n1\n",
    "        off_x = off_0[:, None] * n1 + off_1[None, :]\n",
    "        msk_x = msk_0[:, None] & msk_1[None, :]\n",
    "        x = tl.load(x_ptr + off_x, msk_x, -float('inf'))\n",
    "        x_max = tl.max(x, axis=1, keep_dims=1)\n",
    "        _max = tl.where(_max > x_max, _max, x_max)\n",
    "\n",
    "    tot = tl.zeros((b0, 1),  dtype=tl.float32)\n",
    "    for i_1 in tl.range(tl.cdiv(n1, b1)):\n",
    "        off_1 = i_1 * b1 + tl.arange(0, b1)\n",
    "        msk_1 = off_1 < n1\n",
    "        off_x = off_0[:, None] * n1 + off_1[None, :]\n",
    "        msk_x = msk_0[:, None] & msk_1[None, :]\n",
    "        x = tl.load(x_ptr + off_x, msk_x, -float('inf')) # (b0, b1)\n",
    "        x = x - _max\n",
    "        exp_x = tl.exp2(log2_e * x)\n",
    "        tot += tl.sum(exp_x, axis=1, keep_dims=True)\n",
    "        tl.store(z_ptr + off_x, exp_x, msk_x)\n",
    "        \n",
    "    for i_1 in tl.range(tl.cdiv(n1, b1)):\n",
    "        off_1 = i_1 * b1 + tl.arange(0, b1)\n",
    "        msk_1 = off_1 < n1\n",
    "        off_x = off_0[:, None] * n1 + off_1[None, :]\n",
    "        msk_x = msk_0[:, None] & msk_1[None, :]\n",
    "        exp_x = tl.load(z_ptr + off_x, msk_x) # (b0, b1)\n",
    "        z = exp_x / tot\n",
    "        tl.store(z_ptr + off_x, z, msk_x)\n",
    "\n",
    "def softmax(x, batch_width=2, t_width=16):\n",
    "    b, t = x.shape\n",
    "    z = torch.empty(b, t, device=x.device, dtype=x.dtype)\n",
    "    softmax_k[(cdiv(b, batch_width),)](\n",
    "        x, z,\n",
    "        b, t,\n",
    "        b0=batch_width, b1=t_width\n",
    "    )\n",
    "    return z\n",
    "x = torch.randn(4, 20).float()\n",
    "z_gt = softmax_gt(x)\n",
    "z = softmax(x)\n",
    "print(\"=== pytorch ===\")\n",
    "print(z_gt[:2, :8])\n",
    "print(\"\\n=== softmax_k ===\")\n",
    "print(z[:2, :8])\n",
    "torch.testing.assert_close(z, z_gt)\n",
    "print(\"Passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac91293-8ba3-4286-843c-6d1e5bc88ba3",
   "metadata": {},
   "source": [
    "### Online Softmax\n",
    "\n",
    "Similar to flash attention softmax, this uses the property:\n",
    "\n",
    "$$\n",
    "$$\n",
    "\n",
    "to only perform two loops through the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28347fac-4fab-45fc-ba27-424ecedfec5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== pytorch ===\n",
      "tensor([[0.0653, 0.0214, 0.0079, 0.0573, 0.1437, 0.1421, 0.0149, 0.0141],\n",
      "        [0.0207, 0.0473, 0.0204, 0.0436, 0.0662, 0.2507, 0.0162, 0.1934]])\n",
      "\n",
      "=== softmax_k ===\n",
      "tensor([[0.0653, 0.0214, 0.0079, 0.0573, 0.1437, 0.1421, 0.0149, 0.0141],\n",
      "        [0.0207, 0.0473, 0.0204, 0.0436, 0.0662, 0.2507, 0.0162, 0.1934]])\n",
      "Passed!\n"
     ]
    }
   ],
   "source": [
    "@triton.jit\n",
    "def softmax_k(x_ptr, z_ptr, n0, n1, b0: tl.constexpr, b1: tl.constexpr):\n",
    "    log2_e = 1.44269504\n",
    "    pid_0 = tl.program_id(0)\n",
    "    off_0 = pid_0 * b0 + tl.arange(0, b0)\n",
    "    msk_0 = off_0 < n0\n",
    "\n",
    "    _max = tl.full((b0, 1), -float('inf'), dtype=tl.float32)\n",
    "    _sum = tl.zeros((b0, 1), dtype=tl.float32)\n",
    "    for i_1 in tl.range(tl.cdiv(n1, b1)):\n",
    "        off_1 = i_1 * b1 + tl.arange(0, b1)\n",
    "        msk_1 = off_1 < n1\n",
    "        off_x = off_0[:, None] * n1 + off_1[None, :]\n",
    "        msk_x = msk_0[:, None] & msk_1[None, :]\n",
    "\n",
    "        x = tl.load(x_ptr + off_x, msk_x, -float('inf'))\n",
    "        x_max = tl.max(x, axis=1, keep_dims=1)\n",
    "\n",
    "        old_x_sum_scale = tl.exp2(log2_e * (_max - x_max))\n",
    "        _sum = tl.where(x_max > _max, _sum * old_x_sum_scale, _sum)\n",
    "        _max = tl.where(x_max > _max, x_max, _max)\n",
    "\n",
    "        x_sum = tl.sum(tl.exp2(log2_e * (x - _max)), axis=1, keep_dims=True)\n",
    "        _sum = _sum + x_sum\n",
    "\n",
    "    for i_1 in tl.range(tl.cdiv(n1, b1)):\n",
    "        off_1 = i_1 * b1 + tl.arange(0, b1)\n",
    "        msk_1 = off_1 < n1\n",
    "        off_x = off_0[:, None] * n1 + off_1[None, :]\n",
    "        msk_x = msk_0[:, None] & msk_1[None, :]\n",
    "        x = tl.load(x_ptr + off_x, msk_x, 0) # (b0, b1)\n",
    "        z = tl.exp2(log2_e * (x - _max)) / _sum\n",
    "        tl.store(z_ptr + off_x, z, msk_x)\n",
    "\n",
    "x = torch.randn(4, 20).float()\n",
    "z_gt = softmax_gt(x)\n",
    "z = softmax(x)\n",
    "print(\"=== pytorch ===\")\n",
    "print(z_gt[:2, :8])\n",
    "print(\"\\n=== softmax_k ===\")\n",
    "print(z[:2, :8])\n",
    "torch.testing.assert_close(z, z_gt)\n",
    "print(\"Passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43a9b6b-af1e-4267-b50c-97f517b71962",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28a9015-0d39-435d-bf04-826ee8c43167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fbc26f-2ebe-49b5-902a-aedaadf968a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ac17b4-6559-4441-9647-d29f061bc647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1cd578-4564-43ab-b3de-5f2494eb28cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971f3170-4c3c-4ace-a10c-e5d26555f2f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba9f1ed-ad84-4054-97f1-f856007c9b2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c66f322-f08c-4fd1-83be-006c0bc033d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "from torch import Tensor\n",
    "import triton.language as tl\n",
    "import jaxtyping\n",
    "from jaxtyping import Float32, Int32\n",
    "\n",
    "# @title Setup\n",
    "\n",
    "import triton_viz\n",
    "import inspect\n",
    "from triton_viz.interpreter import record_builder\n",
    "\n",
    "def test(puzzle, puzzle_spec, nelem={}, B={\"B0\": 32}, viz=False):\n",
    "    B = dict(B)\n",
    "    if \"N1\" in nelem and \"B1\" not in B:\n",
    "        B[\"B1\"] = 32\n",
    "    if \"N2\" in nelem and \"B2\" not in B:\n",
    "        B[\"B2\"] = 32\n",
    "\n",
    "    triton_viz.interpreter.record_builder.reset()\n",
    "    torch.manual_seed(0)\n",
    "    signature = inspect.signature(puzzle_spec)\n",
    "    args = {}\n",
    "    for n, p in signature.parameters.items():\n",
    "        print(p)\n",
    "        args[n + \"_ptr\"] = ([d.size for d in p.annotation.dims], p)\n",
    "    args[\"z_ptr\"] = ([d.size for d in signature.return_annotation.dims], None)\n",
    "\n",
    "    tt_args = []\n",
    "    for k, (v, t) in args.items():\n",
    "        tt_args.append(torch.rand(*v) - 0.5)\n",
    "        if t is not None and t.annotation.dtypes[0] == \"int32\":\n",
    "            tt_args[-1] = torch.randint(-100000, 100000, v)\n",
    "    grid = lambda meta: (triton.cdiv(nelem[\"N0\"], meta[\"B0\"]),\n",
    "                         triton.cdiv(nelem.get(\"N1\", 1), meta.get(\"B1\", 1)),\n",
    "                         triton.cdiv(nelem.get(\"N2\", 1), meta.get(\"B2\", 1)))\n",
    "\n",
    "    for k, v in args.items():\n",
    "       print(k, v)\n",
    "    triton_viz.trace(puzzle)[grid](*tt_args, **B, **nelem)\n",
    "    z = tt_args[-1]\n",
    "    tt_args = tt_args[:-1]\n",
    "    z_ = puzzle_spec(*tt_args)\n",
    "    match = torch.allclose(z, z_, rtol=1e-3, atol=1e-3)\n",
    "    print(\"Results match:\",  match)\n",
    "    failures = False\n",
    "    if viz:\n",
    "        failures = triton_viz.launch()\n",
    "    if not match or failures:\n",
    "        print(\"Invalid Access:\", failures)\n",
    "        print(\"Yours:\", z)\n",
    "        print(\"Spec:\", z_)\n",
    "        print(torch.isclose(z, z_))\n",
    "        return\n",
    "    # PUPPIES!\n",
    "    from IPython.display import HTML\n",
    "    import random\n",
    "    print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceab44d6-0aff-4f47-8283-6769356982ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
