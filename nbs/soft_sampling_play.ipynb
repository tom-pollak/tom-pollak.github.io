{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fec8c7c-5c5b-4582-be0d-2aac483d46f1",
   "metadata": {},
   "source": [
    "# Sampling From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f7f433c-c671-4122-a066-765af2bb0490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, tensor\n",
    "from jaxtyping import Float, Int\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "torch.set_grad_enabled(False); # disable backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3a212d0-a163-4b64-b42d-1a4698064fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n"
     ]
    }
   ],
   "source": [
    "local_files_only = True\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n",
    "\n",
    "# model_name = \"gpt2\"\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, local_files_only=local_files_only).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, local_files_only=local_files_only)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9e6633-b55a-4a90-abd9-3050bebf56e9",
   "metadata": {},
   "source": [
    "LLMs do not operate on words -- each word is converted into a high dimensional vector that contains information that gets passed through the model. At each layer, the model reads the vector, performs some computation (attention or MLP) and writes it back to the vector.\n",
    "\n",
    "We call this vector the **residual stream**. To initially create these vectors from a sentence, we have a large lookup table of each \"word\" (or sub-word, see [here](TODO) for more info) to a this high dimensional vector.\n",
    "\n",
    "> We call each \"word\" a **token**.  \n",
    "> You can imagine `token ~= word`\n",
    "\n",
    "This is 768 dimensions on GPT2, and can also be thought of the _width_ of the model\n",
    "\n",
    "_depth_ being the number of layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafde6a7-d46b-4cce-8da9-8889062e2fac",
   "metadata": {},
   "source": [
    "We look up each word in an _embedding_ table. This is a map of 50,000 words to a high dimensional embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6cd4bee-7430-4552-8245-84c825968f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(128256, 2048)\n"
     ]
    }
   ],
   "source": [
    "if model_name == \"gpt2\":\n",
    "    W_E = model.transformer.wte\n",
    "else: # llama\n",
    "    W_E = model.model.embed_tokens\n",
    "print(W_E)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6b5b79-0710-4d7c-b1cb-8fc5d0144cea",
   "metadata": {},
   "source": [
    "Let's see the first 10 dimensions of the token (word) 9246"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2a10d26-abc6-4dfc-b158-647aa703e9d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0378,  0.0211,  0.0037,  0.0184,  0.0267, -0.0135,  0.0104, -0.0383,\n",
       "         0.0137, -0.0064], device='mps:0', requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = 9246\n",
    "first_n_dimensions = 10\n",
    "W_E.weight[token, :first_n_dimensions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6724c01-a02a-4b63-923e-fb56f0ca1b00",
   "metadata": {},
   "source": [
    "And to find the corresponding string word associated with token 9246:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa29f7a6-3f24-40be-9146-1baf44484c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoded token: ' strings'\n"
     ]
    }
   ],
   "source": [
    "def decode(tokens) -> str:\n",
    "    return tokenizer.decode(tokens, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "print(f\"decoded token: {repr(decode(token))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e2a6f3-78dc-4ec1-a59a-0009c3cf7388",
   "metadata": {},
   "source": [
    "Using the `tokenize` and `decode` functions, we can convert back and forth between a string and the initial model vectors (\"embeddings\").\n",
    "\n",
    "Notably the model adds a \"batch\" dimension to the input, which allows us to process multiple inputs at the same time, imagine this allows us to run \"the cat sat on the mat\" and \"I took my dog for a walk\" at the _same time_.\n",
    "\n",
    "Input to a LLM is a list of tokens, which we call length sequence length (or `seq` / `T` (for time dimesion) for short.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "966417d7-0252-4cc6-9d13-85f1a3866db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context length: 131072\n"
     ]
    }
   ],
   "source": [
    "if model_name == \"gpt2\":\n",
    "    context_length = model.config.n_ctx\n",
    "else: # llama\n",
    "    context_length = model.config.max_position_embeddings\n",
    "print(f\"context length: {context_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a70346c1-7659-409f-b128-c3b24148e267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# prompt\n",
      "the cat sat on a mat\n",
      "\n",
      "# tokens shape: (1, 7)\n",
      "[[128000, 1820, 8415, 7731, 389, 264, 5634]]\n",
      "\n",
      "# decoded\n",
      "the cat sat on a mat\n",
      "\n",
      "# embeddings shape: (1, 7, 2048)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize(input) -> Int[Tensor, \"bs seq\"]:\n",
    "    return tokenizer(\n",
    "        input,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=context_length\n",
    "    )[\"input_ids\"].to(device)\n",
    "\n",
    "prompt = 'the cat sat on a mat'\n",
    "tokens = tokenize(prompt)\n",
    "embeddings = W_E.weight[tokens]\n",
    "\n",
    "decoded = decode(tokens[0])\n",
    "\n",
    "print(f\"\"\"\\\n",
    "# prompt\n",
    "{prompt}\n",
    "\n",
    "# tokens shape: {tuple(tokens.shape)}\n",
    "{tokens.tolist()}\n",
    "\n",
    "# decoded\n",
    "{decoded}\n",
    "\n",
    "# embeddings shape: {tuple(embeddings.shape)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74705ade-01f9-41e6-8b3d-efe6e39acd8e",
   "metadata": {},
   "source": [
    "## Output\n",
    "\n",
    "Now given the prompt input, lets run the tokens through the model and look at the output. These are called **logits**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86f0611e-c430-4281-9ed4-05896ee77f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tokens ((1, 7))\n",
      "\n",
      "# Logit Output ((1, 7, 128256))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logits = model(tokens).logits\n",
    "\n",
    "print(f\"\"\"\\\n",
    "# Tokens ({tuple(tokens.shape)})\n",
    "\n",
    "# Logit Output ({tuple(logits.shape)})\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99055bb-2211-4ddd-8ba8-884970cb6a28",
   "metadata": {},
   "source": [
    "The input has shape, `(batch size, sequence length)`, with output `(batch size, sequence length, logits)`\n",
    "\n",
    "For each token in the sequence, the model outputs a score for _every next token_ (50K) representing how likely that token is to come next.\n",
    "\n",
    "For each token, we can see which token the model predicted as _most likely_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83193811-58a3-4570-a4b8-82988e633b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'' => 'Question'\n",
      "'the' => ' '\n",
      "'the cat' => ' is'\n",
      "'the cat sat' => ' on'\n",
      "'the cat sat on' => ' the'\n",
      "'the cat sat on a' => ' hot'\n",
      "'the cat sat on a mat' => '\\n'\n"
     ]
    }
   ],
   "source": [
    "for i in range(tokens.shape[1]):\n",
    "    inp = decode(tokens[0, :i+1])\n",
    "    pred = decode(logits[0, i].argmax())\n",
    "    print(f\"{repr(decode(tokens[0, :i+1]))} => {repr(pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4c9d83-388a-4197-b755-5b61904f900d",
   "metadata": {},
   "source": [
    "So to continue generating tokens, we need to run an **auto regressive** function, that selects a token from the _last_ word in the sequence, and append it to the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84bdb47a-4d6e-48c0-a9c3-4bd0d3ea8647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "the cat sat on a mat\n",
      "\n",
      "---\n",
      "the cat sat on a mat\n",
      "the\n",
      "---\n",
      "the cat sat on a mat\n",
      "the cat\n",
      "---\n",
      "the cat sat on a mat\n",
      "the cat sat\n",
      "---\n",
      "the cat sat on a mat\n",
      "the cat sat on\n",
      "---\n",
      "the cat sat on a mat\n",
      "the cat sat on a\n",
      "---\n",
      "the cat sat on a mat\n",
      "the cat sat on a mat\n",
      "---\n",
      "the cat sat on a mat\n",
      "the cat sat on a mat\n",
      "\n",
      "---\n",
      "the cat sat on a mat\n",
      "the cat sat on a mat\n",
      "the\n",
      "---\n",
      "the cat sat on a mat\n",
      "the cat sat on a mat\n",
      "the cat\n",
      "---\n",
      "the cat sat on a mat\n",
      "the cat sat on a mat\n",
      "the cat sat\n",
      "---\n",
      "the cat sat on a mat\n",
      "the cat sat on a mat\n",
      "the cat sat on\n",
      "---\n",
      "the cat sat on a mat\n",
      "the cat sat on a mat\n",
      "the cat sat on a\n",
      "---\n",
      "the cat sat on a mat\n",
      "the cat sat on a mat\n",
      "the cat sat on a mat\n",
      "---\n",
      "the cat sat on a mat\n",
      "the cat sat on a mat\n",
      "the cat sat on a mat\n",
      "\n",
      "---\n",
      "the cat sat on a mat\n",
      "the cat sat on a mat\n",
      "the cat sat on a mat\n",
      "the\n",
      "---\n",
      "the cat sat on a mat\n",
      "the cat sat on a mat\n",
      "the cat sat on a mat\n",
      "the cat\n",
      "---\n",
      "the cat sat on a mat\n",
      "the cat sat on a mat\n",
      "the cat sat on a mat\n",
      "the cat sat\n",
      "---\n",
      "the cat sat on a mat\n",
      "the cat sat on a mat\n",
      "the cat sat on a mat\n",
      "the cat sat on\n",
      "---\n",
      "the cat sat on a mat\n",
      "the cat sat on a mat\n",
      "the cat sat on a mat\n",
      "the cat sat on a\n"
     ]
    }
   ],
   "source": [
    "def generate(prompt, num_tokens, verbose=False):\n",
    "    tokens = tokenize(prompt)\n",
    "    for i in range(num_tokens):\n",
    "        logits = model(tokens).logits[0, -1] # get the scores of the final token [shape: (n_vocab)]\n",
    "        next_token = logits.argmax(keepdim=True) # pick the largest one\n",
    "        tokens = torch.cat([ tokens, next_token[None] ], dim=1) # concatenate to the current text\n",
    "        if verbose:\n",
    "            print(\"---\")\n",
    "            print(decode(tokens[0]))\n",
    "    return decode(tokens[0])\n",
    "\n",
    "generate(prompt, num_tokens=20, verbose=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3c58eb-5bce-4710-9081-76ea79bacf6a",
   "metadata": {},
   "source": [
    "## Sampling Probability Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660d4005-6523-42d5-8119-a54f515be62f",
   "metadata": {},
   "source": [
    "But just picking the most likely can give quite bland output\n",
    "\n",
    "This takes the model output (which can be any number) and create a _probability distribution_ such that all the scores add up to 1.\n",
    "\n",
    "To do this we use the **softmax** function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a23a75b2-0f0d-45d9-b51a-b9bd78fb37c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the cat sat on a mat from\n",
      "the cat sat on a mat from pee\n",
      "the cat sat on a mat from pee\n",
      "\n",
      "the cat sat on a mat from pee\n",
      "The\n",
      "the cat sat on a mat from pee\n",
      "The owner\n",
      "the cat sat on a mat from pee\n",
      "The owner (\n",
      "the cat sat on a mat from pee\n",
      "The owner (the\n",
      "the cat sat on a mat from pee\n",
      "The owner (the man\n",
      "the cat sat on a mat from pee\n",
      "The owner (the man)\n",
      "the cat sat on a mat from pee\n",
      "The owner (the man) came\n",
      "the cat sat on a mat from pee\n",
      "The owner (the man) came home\n",
      "the cat sat on a mat from pee\n",
      "The owner (the man) came home at\n",
      "the cat sat on a mat from pee\n",
      "The owner (the man) came home at noon\n",
      "the cat sat on a mat from pee\n",
      "The owner (the man) came home at noon (\n",
      "the cat sat on a mat from pee\n",
      "The owner (the man) came home at noon (duck\n",
      "the cat sat on a mat from pee\n",
      "The owner (the man) came home at noon (duck)\n",
      "the cat sat on a mat from pee\n",
      "The owner (the man) came home at noon (duck).\n",
      "the cat sat on a mat from pee\n",
      "The owner (the man) came home at noon (duck).When\n",
      "the cat sat on a mat from pee\n",
      "The owner (the man) came home at noon (duck).When we\n",
      "the cat sat on a mat from pee\n",
      "The owner (the man) came home at noon (duck).When we were\n",
      "the cat sat on a mat from pee\n",
      "The owner (the man) came home at noon (duck).When we were coming\n",
      "the cat sat on a mat from pee\n",
      "The owner (the man) came home at noon (duck).When we were coming home\n",
      "the cat sat on a mat from pee\n",
      "The owner (the man) came home at noon (duck).When we were coming home from\n",
      "the cat sat on a mat from pee\n",
      "The owner (the man) came home at noon (duck).When we were coming home from shopping\n",
      "the cat sat on a mat from pee\n",
      "The owner (the man) came home at noon (duck).When we were coming home from shopping,\n"
     ]
    }
   ],
   "source": [
    "def generate(prompt, num_tokens, verbose=False, seed=42): # add a seed to keep the output deterministic. Try other seeds!\n",
    "    torch.manual_seed(seed)\n",
    "    tokens = tokenize(prompt)\n",
    "    for i in range(num_tokens):\n",
    "        logits = model(tokens).logits[0, -1]\n",
    "        ### New lines\n",
    "        probs = F.softmax(logits, dim=-1) # create probability distribution of scores\n",
    "        next_token = torch.multinomial(probs, 1) # pick a single token from distribution\n",
    "        ###\n",
    "        tokens = torch.cat([ tokens, next_token[None] ], dim=1)\n",
    "        if verbose:\n",
    "            print(decode(tokens[0]))\n",
    "    return decode(tokens[0])\n",
    "\n",
    "generate(prompt, num_tokens=25, verbose=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a881d6-5278-4219-8f39-bc6c5d8c3e81",
   "metadata": {},
   "source": [
    "This already gives a much more interesting output! But perhaps we want to control \n",
    "\n",
    "Now how can we _control_ how much of the distribution we sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946758b1-e393-4a43-b3e9-2876612250ae",
   "metadata": {},
   "source": [
    "## Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04e1a8f-6752-4383-9269-b945712c3983",
   "metadata": {},
   "source": [
    "**Temperature** controls how the distribution is sampled. It's best shown in the context of the examples above\n",
    "\n",
    "- Temperature 0: Completely flattens the distrubution, all probability is given to the token with the largest score\n",
    "- Temperature 1: Standard softmax distrubution, same as sampling above\n",
    "\n",
    "By increasing the temperature, we increase the chance of a token with a lower probability getting picked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50e439c6-8004-421e-8f5a-c886e8a167ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### 0.0 ###\n",
      "the cat sat on a mat\n",
      "the cat sat on a mat\n",
      "the cat sat on a mat\n",
      "the cat sat on a\n",
      "\n",
      "### 0.2 ###\n",
      "the cat sat on a mat\n",
      "the cat sat on a mat\n",
      "the cat sat on a mat\n",
      "the cat sat on a\n",
      "\n",
      "### 0.4 ###\n",
      "the cat sat on a mat\n",
      "the cat sat on a mat\n",
      "the cat sat on a mat\n",
      "the cat sat on a\n",
      "\n",
      "### 0.6 ###\n",
      "the cat sat on a mat\n",
      "the cat sat on a mat\n",
      "the cat sat on a mat\n",
      "the cat sat on a\n",
      "\n",
      "### 0.8 ###\n",
      "the cat sat on a mat from the beginning of time (the pre-colonial period) (1969-1970)\n",
      "\n",
      "\n",
      "### 1.0 ###\n",
      "the cat sat on a mat from pee\n",
      "The owner (the man) came home at noon (duck).When we were\n",
      "\n",
      "### 1.2 ###\n",
      "the cat sat on a mat from pee over it, (the blood doesn't fade at all! he wasn't using blood pressure\n",
      "\n",
      "### 1.4 ###\n",
      "the cat sat on a mat from pee overмосячна по елеруму_KPapasonêduckdourdanination.The\n",
      "\n",
      "### 1.6 ###\n",
      "the cat sat on a mat(нула共同мосячна постілербалиenorasonêduckisticallyisiemptination.The\n",
      "\n",
      "### 1.8 ###\n",
      "the cat sat on a mat�sнула共同мосячна постілерб_KPenorasonêduckisticallyisiemptination.The\n",
      "\n",
      "### 2.0 ###\n",
      "the cat sat on a mat�sнула共同мосяч_dir@m_chr따_KPenorasonêduckisticallyisiemptination.The\n"
     ]
    }
   ],
   "source": [
    "def generate(prompt, num_tokens, temperature=1.0, seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    tokens = tokenize(prompt)\n",
    "    temperature = max(temperature, 1e-8) # temperature 0 => divide by _very small_ constant\n",
    "    for i in range(num_tokens):\n",
    "        logits = model(tokens).logits[0, -1]\n",
    "        probs = F.softmax(logits / temperature, dim=-1) # divide scores, flattening distribution\n",
    "        next_token = torch.multinomial(probs, 1)\n",
    "        tokens = torch.cat([ tokens, next_token[None] ], dim=1)\n",
    "    return decode(tokens[0])\n",
    "\n",
    "for temp in torch.arange(0, 2.2, 0.2):\n",
    "    print(f\"\\n### {temp.item():.1f} ###\")\n",
    "    print(generate(prompt, num_tokens=20, temperature=temp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919ca839-00ad-4023-b4bb-232b2159475f",
   "metadata": {},
   "source": [
    "As the temperature increases, less likely tokens are predicted, which can lead to more interesting output. Setting the temperature hyperparameter correctly can be key to model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0d95f0-0283-409f-b2b4-0d3d1ab0a324",
   "metadata": {},
   "source": [
    "## Top K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4160cd2-839e-4d31-8dee-0be50b96e438",
   "metadata": {},
   "source": [
    "Another parameter used in sampling is `top_k`. This essentially limits the model predicting too \"wild\" predictions by limiting the probability distribution to the top k results.\n",
    "\n",
    "A.k.a currently we are sampling from the entire distribution of 50,000 tokens. But it makes sense that only the top 50 tokens are reasonable continuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2071da2f-44c0-42aa-a83a-f642042ab0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Temperature 0.0 ###\n",
      "the cat sat on a mat\n",
      "\n",
      "##11\n",
      "\n",
      "##11\n",
      "\n",
      "##11\n",
      "\n",
      "##11\n",
      "\n",
      "##11\n",
      "\n",
      "##11\n",
      "\n",
      "##\n",
      "\n",
      "### Temperature 0.2 ###\n",
      "the cat sat on a mat\n",
      "\n",
      "##11\n",
      "\n",
      "##11\n",
      "\n",
      "##11\n",
      "\n",
      "##11\n",
      "\n",
      "##11\n",
      "\n",
      "##11\n",
      "\n",
      "##\n",
      "\n",
      "### Temperature 0.4 ###\n",
      "the cat sat on a mat of of grass grass\n",
      "\n",
      "thethe cat cat sat sat on on a a mat mat of of grass grass\n",
      "\n",
      "thethe cat cat sat sat on on on on a a mat mat of of\n",
      "\n",
      "### Temperature 0.6 ###\n",
      "the cat sat on a mat of of grass grass\n",
      "\n",
      "thethe cat cat sat sat on on mat mat of of grass grass\n",
      "\n",
      "thethe cat cat sat sat on on mat mat of of grass grass\n",
      "\n",
      "thethe\n",
      "\n",
      "### Temperature 0.8 ###\n",
      "the cat sat on a mat of of grass grass\n",
      "\n",
      "QuestionQuestion:: The The cat cat sat sat on on a a mat mat of of\n",
      "\n",
      "AnswerAnswer::##\n",
      "\n",
      "### Temperature 1.0 ###\n",
      "the cat sat on a mat of of many many stripes stripes of of this this cat cat\n",
      "\n",
      "thethe cat cat sat sat on on a a mat mat of ofmanymany many many much much many many many many of of\n",
      "\n",
      "### Temperature 1.2 ###\n",
      "the cat sat on a mat of of many many stripes stripes of of this this cat cat\n",
      "\n",
      "thethe cat cat sat sat on on a a mat mat of ofmanymany many many much much many many large large stripes stripes\n",
      "\n",
      "### Temperature 1.4 ###\n",
      "the cat sat on a mat of of many many stripes stripes of of this this cat cat\n",
      "\n",
      "thethe cat cat sat sat on on a a mat mat of ofmanymany many many much much many many large large stripes stripes\n",
      "\n",
      "### Temperature 1.6 ###\n",
      "the cat sat on a mat of of many many stripes stripes of of this this cat cat\n",
      "\n",
      "thethe kitty kitty of ofmanymany of ofstristripsps stri stri stripes stripes t t a a h h of of\n",
      "\n",
      "### Temperature 1.8 ###\n",
      "the cat sat on a mat of of many many stripes stripes......withwith...... a a......thethe......somesome......oneone......oneone...\n",
      "...\n",
      "AndAndandandANDAND...\n",
      "...\n",
      "\n",
      "\n",
      "### Temperature 2.0 ###\n",
      "the cat sat on a mat of of many many stripes stripes......withwith...... a a......thethecatcat has has......isis......onon a a,,ofof on on mat mat\n"
     ]
    }
   ],
   "source": [
    "def generate(prompt, num_tokens, temperature=1.0, top_k=50, seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    tokens = tokenize(prompt)\n",
    "    temperature = max(temperature, 1e-8)\n",
    "    for i in range(num_tokens):\n",
    "        logits = model(tokens).logits[0, -1]\n",
    "        if top_k:\n",
    "            logits, idxs = logits.topk(top_k) # Sample only topk tokens\n",
    "        else:\n",
    "            idxs = torch.arange(len(logits), device=device) # All idxs\n",
    "    \n",
    "        probs = F.softmax(logits / temperature, dim=-1)\n",
    "        next_token = idxs[torch.multinomial(probs, 1)] # we use the idxs of topk only\n",
    "        tokens = torch.cat([ tokens, next_token[None] ], dim=1)\n",
    "\n",
    "        tokens = torch.cat([ tokens, next_token[None] ], dim=1)\n",
    "    return decode(tokens[0])\n",
    "\n",
    "for temp in torch.arange(0, 2.2, 0.2):\n",
    "    print(f\"\\n### Temperature {temp.item():.1f} ###\")\n",
    "    print(generate(prompt, num_tokens=20, temperature=temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16345a67-46c3-40b5-8ad8-9d0d8f3d5fdb",
   "metadata": {},
   "source": [
    "You can see at even very high temperatures, the output does not devolve into gibberish."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e4abfc-5075-4d8b-8c93-2a2828fae8c8",
   "metadata": {},
   "source": [
    "## Min P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e56a78-81b3-4150-993a-12915c849f4b",
   "metadata": {},
   "source": [
    "Top K can often be a to naive heuristic for sampling. A more common technique nowdays is to instead dispose of tokens that have too low probability.\n",
    "\n",
    "We do this by computing the fraction of the of the probability of a token compared to the most probable token.\n",
    "\n",
    "A.k.a If the most probable token has 60% proability and we have `min_p = 0.1`, we dispose of all tokens with a probability less than 6%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85109728-88fc-4cba-b3d9-5cadb8faad77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Min P: 0.50 ###\n",
      "the cat sat on a mat\n",
      "I’ve been thinking a lot about the cat sat on a mat. It’s a phrase that\n",
      "\n",
      "### Min P: 0.32 ###\n",
      "the cat sat on a mat\n",
      "A cat sat on a mat. The cat sat on a mat. The cat sat on a\n",
      "\n",
      "### Min P: 0.21 ###\n",
      "the cat sat on a mat.\n",
      "I am so thankful to have the opportunity to write for you! I am looking forward to this\n",
      "\n",
      "### Min P: 0.14 ###\n",
      "the cat sat on a mat on a mat.\n",
      "He is on the mat!\n",
      "He is sitting on the cat.\n",
      "The cat is\n",
      "\n",
      "### Min P: 0.09 ###\n",
      "the cat sat on a mat (it’s not my house).\n",
      "I remember sitting in a room, an office perhaps, and the\n",
      "\n",
      "### Min P: 0.06 ###\n",
      "the cat sat on a mat the\n",
      "What is the the cat sat on a mat the formula? In this regard, how can\n",
      "\n",
      "### Min P: 0.04 ###\n",
      "the cat sat on a mat\n",
      "\n",
      "When you interact with a mat that does not respond to you, you assume that they are a\n",
      "\n",
      "### Min P: 0.02 ###\n",
      "the cat sat on a mat? - When We Have No Clothes, Will That Mean We Have a LUXURY BOWL?\n",
      "\n",
      "### Min P: 0.02 ###\n",
      "the cat sat on a mat? he scared the mat! now, all you lazy dog people! you call that discipline?\n",
      "And\n",
      "\n",
      "### Min P: 0.01 ###\n",
      "the cat sat on a mat? Pipska & Honey the 2016 Venus King Edward! Pipska the big black\n"
     ]
    }
   ],
   "source": [
    "def generate(\n",
    "    prompt,\n",
    "    num_tokens,\n",
    "    temperature=1.0,\n",
    "    top_k=None,\n",
    "    min_p=None,\n",
    "    seed=42\n",
    "):\n",
    "    torch.manual_seed(seed)\n",
    "    tokens = tokenize(prompt)\n",
    "    temperature = max(temperature, 1e-8)\n",
    "    for i in range(num_tokens):\n",
    "        logits = model(tokens).logits[0, -1]\n",
    "        if top_k:\n",
    "            logits, idxs = logits.topk(top_k)\n",
    "        else:\n",
    "            idxs = torch.arange(len(logits), device=device)\n",
    "\n",
    "        # TODO: temperature before or after min_p?\n",
    "        probs = F.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "        if min_p is not None:\n",
    "            mask = probs >= (probs.max() * min_p) \n",
    "            idxs, probs = idxs[mask], probs[mask]\n",
    "\n",
    "        next_token = idxs[torch.multinomial(probs, 1)]\n",
    "        tokens = torch.cat([ tokens, next_token[None] ], dim=1)\n",
    "    return decode(tokens[0])\n",
    "\n",
    "for min_p in reversed(torch.logspace(start=math.log10(0.01), end=math.log10(0.5), steps=10, base=10)):\n",
    "    print(f\"\\n### Min P: {min_p.item():.2f} ###\")\n",
    "    print(generate(prompt, num_tokens=20, temperature=1.5, min_p=min_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d1a612-2d08-422f-ae4b-cf9d2f1a631d",
   "metadata": {},
   "source": [
    "## Frequency Penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d0cab7-b60d-4fac-82cc-3cdc6654d2da",
   "metadata": {},
   "source": [
    "As we've seen at low temperatures, the model has a tendancy to repeat itself. For this we can apply a frequency penalty to discourage the model from predicting the same token again.\n",
    "\n",
    "higher frequency -> higher penalty. If token not in sequence, count will be 0 and no penalty applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "790dbaf8-d589-4c2e-8b52-67736cd885a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Frequency Penalty 0.0 ###\n",
      "the cat sat on a mat\n",
      "the cat sat on a mat\n",
      "the cat sat on a mat\n",
      "the cat sat on a\n",
      "\n",
      "### Frequency Penalty 0.2 ###\n",
      "the cat sat on a mat\n",
      "the cat sat on a mat\n",
      "the cat sat on a mat\n",
      "the cat sat on a\n",
      "\n",
      "### Frequency Penalty 0.4 ###\n",
      "the cat sat on a mat\n",
      "the cat sat on a mat\n",
      "The cat sat on a mat. The cat sat on a\n",
      "\n",
      "### Frequency Penalty 0.6 ###\n",
      "the cat sat on a mat\n",
      "The cat sat on a mat. The cat sat on a mat. The cat sat on a\n",
      "\n",
      "### Frequency Penalty 0.8 ###\n",
      "the cat sat on a mat\n",
      "The cat sat on a mat. The cat sat on the mat. The cat sat on the\n",
      "\n",
      "### Frequency Penalty 1.0 ###\n",
      "the cat sat on a mat\n",
      "The cat sat on a mat. The cat was very happy. The mouse was very sad.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate(\n",
    "    prompt,\n",
    "    num_tokens,\n",
    "    temperature=1.0,\n",
    "    top_k=None,\n",
    "    min_p=None,\n",
    "    frequency_penalty=None,\n",
    "    seed=42,\n",
    "):\n",
    "    torch.manual_seed(seed)\n",
    "    tokens = tokenize(prompt)\n",
    "    temperature = max(temperature, 1e-8)\n",
    "    for i in range(num_tokens):\n",
    "        logits = model(tokens).logits[0, -1]\n",
    "\n",
    "        if frequency_penalty:\n",
    "            *_, vocab_size = logits.shape\n",
    "            # get frequency of each of the logits in the current output\n",
    "            id_freqs = torch.bincount(tokens[0], minlength=vocab_size)\n",
    "            logits -= frequency_penalty * id_freqs\n",
    "\n",
    "        if top_k:\n",
    "            logits, idxs = logits.topk(top_k)\n",
    "        else:\n",
    "            idxs = torch.arange(len(logits), device=device)\n",
    "\n",
    "        # TODO: temperature before or after min_p?\n",
    "        probs = F.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "        if min_p is not None:\n",
    "            mask = probs >= (probs.max() * min_p) \n",
    "            idxs, probs = idxs[mask], probs[mask]\n",
    "\n",
    "        next_token = idxs[torch.multinomial(probs, 1)]\n",
    "        tokens = torch.cat([ tokens, next_token[None] ], dim=1)\n",
    "    return decode(tokens[0])\n",
    "\n",
    "for freq_penalty in torch.linspace(start=0, end=1., steps=6):\n",
    "    print(f\"\\n### Frequency Penalty {freq_penalty.item():.1f} ###\")\n",
    "    print(generate(prompt, num_tokens=20, temperature=0., frequency_penalty=freq_penalty))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f1ce28-f469-49fa-a998-dc959e7418e7",
   "metadata": {},
   "source": [
    "## Soft Sampling\n",
    "\n",
    "GPT2 has tied embeddings, so this should be easy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e59f10f1-a120-452b-8151-379ef64f20e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    prompt,\n",
    "    num_tokens,\n",
    "    temperature=1.0,\n",
    "    top_k=None,\n",
    "    min_p=None,\n",
    "    frequency_penalty=None,\n",
    "    soft_sample=False,\n",
    "    seed=42,\n",
    "):\n",
    "    torch.manual_seed(seed)\n",
    "    tokens = tokenize(prompt)\n",
    "    inputs_embeds = W_E(tokens)\n",
    "    temperature = max(temperature, 1e-8)\n",
    "    for i in range(num_tokens):\n",
    "        logits = model(inputs_embeds=inputs_embeds).logits[0, -1]\n",
    "\n",
    "        if frequency_penalty:\n",
    "            *_, vocab_size = logits.shape\n",
    "            # get frequency of each of the logits in the current output\n",
    "            id_freqs = torch.bincount(tokens[0], minlength=vocab_size)\n",
    "            logits -= frequency_penalty * id_freqs\n",
    "\n",
    "        if top_k:\n",
    "            logits, idxs = logits.topk(top_k)\n",
    "        else:\n",
    "            idxs = torch.arange(len(logits), device=device)\n",
    "\n",
    "        # TODO: temperature before or after min_p?\n",
    "        probs = F.softmax(logits / temperature, dim=-1)\n",
    "        \n",
    "        if min_p is not None:\n",
    "            mask = probs >= (probs.max() * min_p) \n",
    "            idxs, probs = idxs[mask], probs[mask]\n",
    "\n",
    "        next_token = idxs[torch.multinomial(probs, 1)]\n",
    "        tokens = torch.cat([ tokens, next_token[None] ], dim=1)\n",
    "\n",
    "        if soft_sample:\n",
    "            # tied embeddings\n",
    "            next_embed = probs[None] @ W_E.weight[idxs]\n",
    "        else:\n",
    "            next_embed = W_E(next_token)\n",
    "            \n",
    "        inputs_embeds = torch.cat( [ inputs_embeds, next_embed[None] ], dim=1)\n",
    "    \n",
    "    return decode(tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38a6801a-2102-4136-a5e9-25636a5c4d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==== SOFT SAMPLE: False ====\n",
      "\n",
      "### Min P: 0.50 ###\n",
      "the cat sat on a mat\n",
      "I have been reading a lot about the importance of being present. It’s not just for meditation\n",
      "\n",
      "### Min P: 0.32 ###\n",
      "the cat sat on a mat\n",
      "A cat sat on a mat. The cat was very pleased with herself. She had got her\n",
      "\n",
      "### Min P: 0.21 ###\n",
      "the cat sat on a mat\n",
      "A cat sat on a mat. The cat was very fat, but it wasn't very happy\n",
      "\n",
      "### Min P: 0.14 ###\n",
      "the cat sat on a mat\n",
      "A cat sat on a mat. The cat sat there for 5 minutes, then jumped up\n",
      "\n",
      "### Min P: 0.09 ###\n",
      "the cat sat on a mat.\n",
      "A little girl said, \"Mama, can I have a cookie?\" Her mother replied,\n",
      "\n",
      "### Min P: 0.06 ###\n",
      "the cat sat on a mat that was not warm enough, so I put him in the freezer, which is why he’s not\n",
      "\n",
      "### Min P: 0.04 ###\n",
      "the cat sat on a mat is cute!\n",
      "I don't know what it is about cats that I love, but I just do\n",
      "\n",
      "### Min P: 0.02 ###\n",
      "the cat sat on a mat (an example of what the cat could eat if she chose to) with the child's favourite food\n",
      "\n",
      "### Min P: 0.02 ###\n",
      "the cat sat on a mat to look cute but never ate.\n",
      "Cat Cat Can't Eat Mat!Wanted: the\n",
      "\n",
      "### Min P: 0.01 ###\n",
      "the cat sat on a mat in the lap of mother, and stared up at her as she\n",
      "laid her hands upon it\n",
      "\n",
      "\n",
      "==== SOFT SAMPLE: True ====\n",
      "\n",
      "### Min P: 0.50 ###\n",
      "the cat sat on a mat\n",
      "\n",
      " the u o n mat\n",
      " ( / Mat t is sat p kdef        t\n",
      "\n",
      "### Min P: 0.32 ###\n",
      "the cat sat on a mat\n",
      " the. u o matat ( is mat / tadef kQuestion papp\n",
      "\n",
      "### Min P: 0.21 ###\n",
      "the cat sat on a mat\n",
      " the . �\n",
      "\n",
      ". ( - l h ih t l ilk_l,\n",
      "\n",
      "### Min P: 0.14 ###\n",
      "the cat sat on a mat\n",
      "Ian  [, \n",
      " n l tr t e. h c k  thich\n",
      "\n",
      "### Min P: 0.09 ###\n",
      "the cat sat on a mat.\n",
      " Int ,) t\n",
      " y is / l l,\n",
      " ( v th tev\n",
      "\n",
      "### Min P: 0.06 ###\n",
      "the cat sat on a mat that Theatap b.\n",
      " d m v g l (, v t u o e \n",
      "\n",
      "### Min P: 0.04 ###\n",
      "the cat sat on a mat is Heong bk x, th w i n\n",
      " I, ( t The y e \n",
      "\n",
      "### Min P: 0.02 ###\n",
      "the cat sat on a mat ( And &af edited, the sat at is -\n",
      " y ley el st ot e ke\n",
      "\n",
      "### Min P: 0.02 ###\n",
      "the cat sat on a mat towe  There ( is | LE.\n",
      " the /. \". n. H - I.\n",
      "\n",
      "### Min P: 0.01 ###\n",
      "the cat sat on a mat inIniciair every a matTheSee.\n",
      " one y t,, It\n",
      " o g.\n"
     ]
    }
   ],
   "source": [
    "for soft_sample in False, True:\n",
    "    print(f\"\\n\\n==== SOFT SAMPLE: {soft_sample} ====\")\n",
    "    for min_p in reversed(torch.logspace(start=math.log10(0.01), end=math.log10(0.5), steps=10, base=10)):\n",
    "        print(f\"\\n### Min P: {min_p.item():.2f} ###\")\n",
    "        print(generate(prompt, num_tokens=20, temperature=1., min_p=min_p, frequency_penalty=1., soft_sample=soft_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8295968-8148-4f61-8812-a55cf4e416a0",
   "metadata": {},
   "source": [
    "## Mock Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9beff69-2650-43bc-94b8-45ec54d67b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 10.2525\n",
      "---\n",
      "Question\n",
      " (y the cat the\n",
      "---\n",
      "def\n",
      " you\n",
      " two\n",
      " where\n",
      "---\n",
      "Question  is number\n",
      "inic\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from fastcore.meta import delegates\n",
    "\n",
    "def mk_proba_dist(\n",
    "    logits, # (batch_size, d_vocab)\n",
    "    temperature=1.0,\n",
    "    top_k=None,\n",
    "    min_p=None,\n",
    "):\n",
    "    batch_size, d_vocab = logits.shape\n",
    "    device = logits.device\n",
    "    if top_k:\n",
    "        logits, idxs = logits.topk(top_k, dim=-1)\n",
    "    else:\n",
    "        idxs = (\n",
    "            torch.arange(d_vocab, device=device)\n",
    "            .repeat(batch_size)\n",
    "            .reshape(batch_size, d_vocab)\n",
    "        )\n",
    "\n",
    "    # TODO: temperature before or after min_p?\n",
    "    probs = F.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "    if min_p is not None:\n",
    "        max_probs = probs.max(dim=-1, keepdim=True).values\n",
    "        threshold = max_probs * min_p\n",
    "        mask = probs >= threshold\n",
    "        probs = probs * mask\n",
    "        probs = probs / probs.sum(dim=-1, keepdim=True) # renormalize\n",
    "        idxs = idxs * mask\n",
    "    return idxs, probs\n",
    "\n",
    "@delegates(mk_proba_dist)\n",
    "def soft_sampling_train_step(\n",
    "    model,\n",
    "    batch, # tokens of shape (batch_size, seq_len)\n",
    "    W_E, # model's embedding matrix\n",
    "    guidance_alpha, # guidance weighting -- 1 equivalent to discrete sampling\n",
    "    **kwargs, # passed to mk_proba_dist\n",
    "):\n",
    "    \"Single train step using soft sampling\"\n",
    "    assert 0 <= guidance_alpha <= 1\n",
    "    batch_size, seq_len = batch.shape\n",
    "    device = batch.device\n",
    "\n",
    "    # cache\n",
    "    past_key_values = None\n",
    "    position_ids = torch.zeros(batch_size, 1, dtype=torch.long, device=device)\n",
    "\n",
    "\n",
    "    loss = torch.tensor(0., device=device)\n",
    "    embeds = W_E[batch[:, :1]]  # BOS shape: (batch_size, 1, d_model)\n",
    "    tokens = [ batch[:, :1].detach().cpu() ]\n",
    "    for t in range(1, seq_len):\n",
    "        outputs = model(\n",
    "            inputs_embeds=embeds,\n",
    "            past_key_values=past_key_values,\n",
    "            position_ids=position_ids,\n",
    "            use_cache=True\n",
    "        )\n",
    "\n",
    "        logits_t = outputs.logits[:, -1]\n",
    "        past_key_values = outputs.past_key_values\n",
    "\n",
    "        i_t, p_t = mk_proba_dist(logits_t, **kwargs)\n",
    "\n",
    "        # loss\n",
    "        loss_t = F.cross_entropy(p_t, batch[:, t])\n",
    "        loss += loss_t\n",
    "\n",
    "        # discrete sample -- for logging\n",
    "        indices = torch.multinomial(p_t, 1) # (batch_size, 1)\n",
    "        batch_indices = torch.arange(batch_size)[:, None] # (batch_size, 1)\n",
    "        next_token = i_t[batch_indices, indices].detach().cpu()\n",
    "        tokens.append(next_token)\n",
    "\n",
    "        # soft sample\n",
    "        next_emb_soft = p_t @ W_E      # soft sampling\n",
    "        next_emb_gt = W_E[batch[:, t]] # guidance sampling\n",
    "\n",
    "        next_embed = (\n",
    "            guidance_alpha * next_emb_gt +\n",
    "            (1 - guidance_alpha) * next_emb_soft\n",
    "        )\n",
    "        embeds = torch.cat([embeds, next_embed[:, None, :]], dim=1)\n",
    "        position_ids += 1\n",
    "\n",
    "    if return_tokens:\n",
    "        tokens = torch.cat(tokens, dim=1)\n",
    "    # normalize gradient: sum batch, mean sequence length\n",
    "    loss /= seq_len\n",
    "    return loss, tokens\n",
    "\n",
    "prompts = [\n",
    "    \"the cat sat in the hat\",\n",
    "    \"where did all the hats go?\",\n",
    "    \"I am the rizzler\",\n",
    "]\n",
    "\n",
    "batch = tokenize(prompts)\n",
    "loss, tokens = soft_sampling_train_step(\n",
    "    model,\n",
    "    batch,\n",
    "    W_E.weight,\n",
    "    guidance_alpha=0.5,\n",
    "    return_tokens=True,\n",
    "    min_p=0.1\n",
    ")\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "for token_samp in tokens:\n",
    "    print(\"---\")\n",
    "    print(decode(token_samp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa86779-f05b-4f31-a7dc-c3d6acee7c02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb81d582-36d2-4963-9b81-cef445dc9120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b73b01a-98fc-477c-900d-fa71b3c2aa48",
   "metadata": {},
   "source": [
    "## Broken training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a2af105-ea48-4923-a41c-51f93c4ded89",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "non-default argument 'max_samp_alpha' follows default argument",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m partial\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;129;43m@dataclass\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mTrainConfig\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# training\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_lr\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-3\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_samp_alpha\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.7\u001b[39;49m\n",
      "File \u001b[0;32m~/micromamba/envs/core/lib/python3.11/dataclasses.py:1232\u001b[0m, in \u001b[0;36mdataclass\u001b[0;34m(cls, init, repr, eq, order, unsafe_hash, frozen, match_args, kw_only, slots, weakref_slot)\u001b[0m\n\u001b[1;32m   1229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrap\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;66;03m# We're called as @dataclass without parens.\u001b[39;00m\n\u001b[0;32m-> 1232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/core/lib/python3.11/dataclasses.py:1222\u001b[0m, in \u001b[0;36mdataclass.<locals>.wrap\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap\u001b[39m(\u001b[38;5;28mcls\u001b[39m):\n\u001b[0;32m-> 1222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_process_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munsafe_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mfrozen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatch_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkw_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslots\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mweakref_slot\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/core/lib/python3.11/dataclasses.py:1027\u001b[0m, in \u001b[0;36m_process_class\u001b[0;34m(cls, init, repr, eq, order, unsafe_hash, frozen, match_args, kw_only, slots, weakref_slot)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m init:\n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;66;03m# Does this class have a post-init function?\u001b[39;00m\n\u001b[1;32m   1024\u001b[0m     has_post_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, _POST_INIT_NAME)\n\u001b[1;32m   1026\u001b[0m     _set_new_attribute(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__init__\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m-> 1027\u001b[0m                        \u001b[43m_init_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_init_fields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mstd_init_fields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mkw_only_init_fields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mfrozen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mhas_post_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;66;43;03m# The name to use for the \"self\"\u001b[39;49;00m\n\u001b[1;32m   1033\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;66;43;03m# param in __init__.  Use \"self\"\u001b[39;49;00m\n\u001b[1;32m   1034\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;66;43;03m# if possible.\u001b[39;49;00m\n\u001b[1;32m   1035\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m__dataclass_self__\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mself\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m                                        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mself\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mslots\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m                      \u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;66;03m# Get the fields as a list, and include only real fields.  This is\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;66;03m# used in all of the following methods.\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m field_list \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fields\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39m_field_type \u001b[38;5;129;01mis\u001b[39;00m _FIELD]\n",
      "File \u001b[0;32m~/micromamba/envs/core/lib/python3.11/dataclasses.py:545\u001b[0m, in \u001b[0;36m_init_fn\u001b[0;34m(fields, std_fields, kw_only_fields, frozen, has_post_init, self_name, globals, slots)\u001b[0m\n\u001b[1;32m    543\u001b[0m             seen_default \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;28;01melif\u001b[39;00m seen_default:\n\u001b[0;32m--> 545\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnon-default argument \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    546\u001b[0m                             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfollows default argument\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28mlocals\u001b[39m \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_type_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m: f\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fields}\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m.\u001b[39mupdate({\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMISSING\u001b[39m\u001b[38;5;124m'\u001b[39m: MISSING,\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_HAS_DEFAULT_FACTORY\u001b[39m\u001b[38;5;124m'\u001b[39m: _HAS_DEFAULT_FACTORY,\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dataclass_builtins_object__\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mobject\u001b[39m,\n\u001b[1;32m    553\u001b[0m })\n",
      "\u001b[0;31mTypeError\u001b[0m: non-default argument 'max_samp_alpha' follows default argument"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    # training\n",
    "    max_lr: float = 1e-3\n",
    "    max_samp_alpha: 0.7\n",
    "    lr_warmup_frac: float = 0.1\n",
    "    samp_alpha_warmup_frac: float = 0.5\n",
    "    # model\n",
    "    W_E_k: tuple[str, str]\n",
    "    # sampling\n",
    "    temperature: float = 1.0\n",
    "    top_k: float | None = None\n",
    "    min_p: float | None = None\n",
    "    frequency_penalty: float | None = None\n",
    "    # logging / checkpoint\n",
    "    log_every: int | None = None\n",
    "    # validation\n",
    "    val_every: int | None = None\n",
    "    val_samples: int | None = None\n",
    "\n",
    "    def get_W_E(model):\n",
    "        mod_k, submod_k = W_E_k\n",
    "        return getattr(getattr(model, mod_k), submod_k)\n",
    "\n",
    "class DataLoaders:\n",
    "    def __init__(train_dl, val_dl): self.train, self.val = train_dl, val_dl\n",
    "        \n",
    "def linear_sched(t, max_t):\n",
    "    return t / max_t\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, tokenizer, val_dl, cfg: TrainConfig):\n",
    "    losses = []\n",
    "    samples = []\n",
    "    for step, batch in enumerate(tqdm(val_dl)):\n",
    "        loss, tokens = do_step(model, batch, cfg, return_tokens=True)\n",
    "        if step < cfg.val_samples: samples.append(tokens)\n",
    "        losses.append(loss)\n",
    "    print(f\"Val Loss: {losses.mean().item():.4f}\")\n",
    "\n",
    "def train(model, tokenizer, dls: DataLoaders, cfg: TrainConfig):\n",
    "    W_E = cfg.get_W_E(model)\n",
    "    total_steps = len(dl)\n",
    "    for step, batch in enumerate(tqdm(dls.train)):\n",
    "        lr = cfg.max_lr * linear_sched(step, total_steps)\n",
    "        opt.param_groups[0]['lr'] = lr\n",
    "        opt.zero_grad()\n",
    "        loss, _ = do_step(model, batch, W_E, sched_sample_alpha, return_tokens=False)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        if step % cfg.log_every == 0: print(f\"Train Loss: {loss.item():.4f}\")\n",
    "        if step % cfg.val_every == 0: validate(model, tokenizer, dls.valid, cfg)\n",
    "        \n",
    "\n",
    "cfg = TrainConfig(\n",
    "    W_E_k=(\"model\", \"embed_tokens\")\n",
    ")\n",
    "dls = DataLoaders(train_dl, val_dl)\n",
    "train(model, tokenizer, dls, cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
